<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6d6aa1be033625d201a190fc9c5cbfb4",
  "translation_date": "2025-08-25T22:42:41+00:00",
  "source_file": "6-consumer/lessons/1-speech-recognition/README.md",
  "language_code": "de"
}
-->
# Spracherkennung mit einem IoT-Ger√§t

![Eine Sketchnote-√úbersicht dieser Lektion](../../../../../translated_images/lesson-21.e34de51354d6606fb5ee08d8c89d0222eea0a2a7aaf744a8805ae847c4f69dc4.de.jpg)

> Sketchnote von [Nitya Narasimhan](https://github.com/nitya). Klicken Sie auf das Bild f√ºr eine gr√∂√üere Version.

Dieses Video gibt einen √úberblick √ºber den Azure-Sprachdienst, ein Thema, das in dieser Lektion behandelt wird:

[![Wie man mit der Cognitive Services Speech-Ressource von Microsoft Azure beginnt](https://img.youtube.com/vi/iW0Fw0l3mrA/0.jpg)](https://www.youtube.com/watch?v=iW0Fw0l3mrA)

> üé• Klicken Sie auf das obige Bild, um ein Video anzusehen

## Quiz vor der Lektion

[Quiz vor der Lektion](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/41)

## Einf√ºhrung

‚ÄûAlexa, stelle einen 12-Minuten-Timer.‚Äú

‚ÄûAlexa, Timer-Status.‚Äú

‚ÄûAlexa, stelle einen 8-Minuten-Timer namens Brokkoli d√§mpfen.‚Äú

Smarte Ger√§te werden immer allgegenw√§rtiger. Nicht nur als smarte Lautsprecher wie HomePods, Echos und Google Homes, sondern auch eingebettet in unsere Telefone, Uhren und sogar in Lampenfassungen und Thermostate.

> üíÅ Ich habe mindestens 19 Ger√§te in meinem Zuhause, die Sprachassistenten haben, und das sind nur die, von denen ich wei√ü!

Sprachsteuerung erh√∂ht die Barrierefreiheit, indem sie Menschen mit eingeschr√§nkter Beweglichkeit erm√∂glicht, mit Ger√§ten zu interagieren. Egal, ob es sich um eine dauerhafte Behinderung handelt, wie das Fehlen von Armen, eine vor√ºbergehende Behinderung wie ein gebrochener Arm, oder einfach die H√§nde voll mit Eink√§ufen oder kleinen Kindern ‚Äì die M√∂glichkeit, unser Zuhause mit der Stimme statt mit den H√§nden zu steuern, er√∂ffnet eine Welt voller M√∂glichkeiten. Ein ‚ÄûHey Siri, schlie√üe mein Garagentor‚Äú zu rufen, w√§hrend man mit einem Babywechsel und einem ungest√ºmen Kleinkind besch√§ftigt ist, kann eine kleine, aber effektive Verbesserung des Alltags sein.

Eine der beliebtesten Anwendungen f√ºr Sprachassistenten ist das Stellen von Timern, insbesondere K√ºchentimern. Mehrere Timer nur mit der Stimme einstellen zu k√∂nnen, ist eine gro√üe Hilfe in der K√ºche ‚Äì kein Grund, das Kneten von Teig, das R√ºhren von Suppe oder das Reinigen von Teigtaschenf√ºllung von den H√§nden zu unterbrechen, um einen physischen Timer zu bedienen.

In dieser Lektion lernen Sie, wie Sie Spracherkennung in IoT-Ger√§te integrieren. Sie erfahren, wie Mikrofone als Sensoren funktionieren, wie man Audio von einem Mikrofon, das an ein IoT-Ger√§t angeschlossen ist, aufnimmt, und wie man KI einsetzt, um das Geh√∂rte in Text umzuwandeln. Im weiteren Verlauf dieses Projekts bauen Sie einen smarten K√ºchentimer, der Timer in mehreren Sprachen per Sprachbefehl einstellen kann.

In dieser Lektion behandeln wir:

* [Mikrofone](../../../../../6-consumer/lessons/1-speech-recognition)
* [Audio von Ihrem IoT-Ger√§t aufnehmen](../../../../../6-consumer/lessons/1-speech-recognition)
* [Sprache in Text umwandeln](../../../../../6-consumer/lessons/1-speech-recognition)
* [Sprache in Text konvertieren](../../../../../6-consumer/lessons/1-speech-recognition)

## Mikrofone

Mikrofone sind analoge Sensoren, die Schallwellen in elektrische Signale umwandeln. Vibrationen in der Luft bewegen Komponenten im Mikrofon minimal, was winzige √Ñnderungen in elektrischen Signalen verursacht. Diese √Ñnderungen werden dann verst√§rkt, um ein elektrisches Ausgangssignal zu erzeugen.

### Mikrofontypen

Mikrofone gibt es in verschiedenen Typen:

* Dynamisch ‚Äì Dynamische Mikrofone haben einen Magneten, der an einer beweglichen Membran befestigt ist. Diese bewegt sich in einer Drahtspule und erzeugt so einen elektrischen Strom. Dies ist das Gegenteil der meisten Lautsprecher, die einen elektrischen Strom verwenden, um einen Magneten in einer Drahtspule zu bewegen, wodurch eine Membran Schall erzeugt. Das bedeutet, dass Lautsprecher als dynamische Mikrofone verwendet werden k√∂nnen und umgekehrt. In Ger√§ten wie Gegensprechanlagen, bei denen ein Benutzer entweder zuh√∂rt oder spricht, aber nicht beides gleichzeitig, kann ein Ger√§t sowohl als Lautsprecher als auch als Mikrofon fungieren.

    Dynamische Mikrofone ben√∂tigen keine Stromversorgung, das elektrische Signal wird vollst√§ndig vom Mikrofon erzeugt.

    ![Patti Smith singt in ein Shure SM58 (dynamisches Kardioid-Mikrofon)](../../../../../translated_images/dynamic-mic.8babac890a2d80dfb0874b5bf37d4b851fe2aeb9da6fd72945746176978bf3bb.de.jpg)

* B√§ndchen ‚Äì B√§ndchenmikrofone √§hneln dynamischen Mikrofonen, haben jedoch ein Metallb√§ndchen anstelle einer Membran. Dieses B√§ndchen bewegt sich in einem Magnetfeld und erzeugt einen elektrischen Strom. Wie dynamische Mikrofone ben√∂tigen auch B√§ndchenmikrofone keine Stromversorgung.

    ![Edmund Lowe, amerikanischer Schauspieler, steht 1942 an einem Radiomikrofon (gekennzeichnet f√ºr das (NBC) Blue Network) und h√§lt ein Skript](../../../../../translated_images/ribbon-mic.eacc8e092c7441caee6d7a81e2f40e1675bf36269848964c7c09c9a9acb05127.de.jpg)

* Kondensator ‚Äì Kondensatormikrofone haben eine d√ºnne Metallmembran und eine feste Metallr√ºckplatte. Elektrizit√§t wird auf beide angewendet, und wenn die Membran vibriert, √§ndert sich die statische Ladung zwischen den Platten und erzeugt ein Signal. Kondensatormikrofone ben√∂tigen Strom, um zu funktionieren ‚Äì genannt *Phantomspannung*.

    ![C451B Kleinmembran-Kondensatormikrofon von AKG Acoustics](../../../../../translated_images/condenser-mic.6f6ed5b76ca19e0ec3fd0c544601542d4479a6cb7565db336de49fbbf69f623e.de.jpg)

* MEMS ‚Äì Mikroelektromechanische Systeme (MEMS) sind Mikrofone auf einem Chip. Sie haben eine druckempfindliche Membran, die auf einen Siliziumchip ge√§tzt ist, und funktionieren √§hnlich wie ein Kondensatormikrofon. Diese Mikrofone k√∂nnen winzig sein und in Schaltungen integriert werden.

    ![Ein MEMS-Mikrofon auf einer Leiterplatte](../../../../../translated_images/mems-microphone.80574019e1f5e4d9ee72fed720ecd25a39fc2969c91355d17ebb24ba4159e4c4.de.png)

    Im obigen Bild ist der Chip mit der Aufschrift **LEFT** ein MEMS-Mikrofon mit einer winzigen Membran, die weniger als einen Millimeter breit ist.

‚úÖ Recherchieren Sie: Welche Mikrofone haben Sie in Ihrer Umgebung ‚Äì sei es in Ihrem Computer, Ihrem Telefon, Ihrem Headset oder in anderen Ger√§ten? Welche Mikrofontypen sind das?

### Digitale Audiodaten

Audio ist ein analoges Signal, das sehr feingranulare Informationen tr√§gt. Um dieses Signal in digitale Daten umzuwandeln, muss das Audio viele tausend Male pro Sekunde abgetastet werden.

> üéì Abtastung bedeutet, das Audiosignal in einen digitalen Wert umzuwandeln, der das Signal zu einem bestimmten Zeitpunkt repr√§sentiert.

![Ein Liniendiagramm, das ein Signal mit diskreten Punkten in festen Intervallen zeigt](../../../../../translated_images/sampling.6f4fadb3f2d9dfe7618f9edfe75a350e6b3f74293ec84f02ab69c19d2afe3d73.de.png)

Digitales Audio wird mit Puls-Code-Modulation (PCM) abgetastet. PCM liest die Spannung des Signals aus und w√§hlt den n√§chstgelegenen diskreten Wert zu dieser Spannung basierend auf einer definierten Gr√∂√üe.

> üíÅ Sie k√∂nnen sich PCM als die Sensorversion der Pulsweitenmodulation (PWM) vorstellen (PWM wurde bereits in [Lektion 3 des Einsteigerprojekts](../../../1-getting-started/lessons/3-sensors-and-actuators/README.md#pulse-width-modulation) behandelt). PCM wandelt ein analoges Signal in ein digitales um, w√§hrend PWM ein digitales Signal in ein analoges umwandelt.

Zum Beispiel bieten die meisten Streaming-Musikdienste 16-Bit- oder 24-Bit-Audio an. Das bedeutet, dass sie die Spannung in einen Wert umwandeln, der in einen 16-Bit-Integer oder 24-Bit-Integer passt. 16-Bit-Audio umfasst Werte im Bereich von -32.768 bis 32.767, 24-Bit-Audio reicht von ‚àí8.388.608 bis 8.388.607. Je mehr Bits, desto n√§her ist die Abtastung an dem, was unsere Ohren tats√§chlich h√∂ren.

> üíÅ Sie haben vielleicht schon von 8-Bit-Audio geh√∂rt, oft als LoFi bezeichnet. Dies ist Audio, das nur mit 8-Bit abgetastet wurde, also -128 bis 127. Die ersten Computeraudios waren aufgrund von Hardwarebeschr√§nkungen auf 8-Bit begrenzt, weshalb dies oft in Retro-Gaming vorkommt.

Diese Abtastungen werden viele tausend Male pro Sekunde durchgef√ºhrt, mit gut definierten Abtastraten, die in KHz (tausend Abtastungen pro Sekunde) gemessen werden. Streaming-Musikdienste verwenden f√ºr die meisten Audiodaten 48 KHz, aber einige ‚Äûverlustfreie‚Äú Audiodaten verwenden bis zu 96 KHz oder sogar 192 KHz. Je h√∂her die Abtastrate, desto n√§her ist das Audio am Original ‚Äì bis zu einem gewissen Punkt. Es gibt Diskussionen dar√ºber, ob Menschen Unterschiede oberhalb von 48 KHz wahrnehmen k√∂nnen.

‚úÖ Recherchieren Sie: Wenn Sie einen Streaming-Musikdienst nutzen, welche Abtastrate und -gr√∂√üe verwendet dieser? Wenn Sie CDs verwenden, welche Abtastrate und -gr√∂√üe hat CD-Audio?

Es gibt verschiedene Formate f√ºr Audiodaten. Sie haben wahrscheinlich schon von MP3-Dateien geh√∂rt ‚Äì Audiodaten, die komprimiert wurden, um sie kleiner zu machen, ohne an Qualit√§t zu verlieren. Unkomprimierte Audiodaten werden oft als WAV-Datei gespeichert ‚Äì dies ist eine Datei mit 44 Bytes Header-Informationen, gefolgt von den rohen Audiodaten. Der Header enth√§lt Informationen wie die Abtastrate (z. B. 16000 f√ºr 16 KHz), die Abtastgr√∂√üe (16 f√ºr 16-Bit) und die Anzahl der Kan√§le. Nach dem Header enth√§lt die WAV-Datei die rohen Audiodaten.

> üéì Kan√§le beziehen sich darauf, wie viele verschiedene Audiostr√∂me das Audio ausmachen. Zum Beispiel h√§tte Stereo-Audio mit links und rechts 2 Kan√§le. F√ºr 7.1-Surround-Sound in einem Heimkinosystem w√§ren es 8.

### Gr√∂√üe von Audiodaten

Audiodaten sind relativ gro√ü. Zum Beispiel ben√∂tigt die Aufnahme von unkomprimiertem 16-Bit-Audio bei 16 KHz (eine ausreichende Rate f√ºr die Verwendung mit einem Sprach-zu-Text-Modell) 32 KB Daten pro Sekunde Audio:

* 16-Bit bedeutet 2 Bytes pro Abtastung (1 Byte = 8 Bit).
* 16 KHz sind 16.000 Abtastungen pro Sekunde.
* 16.000 x 2 Bytes = 32.000 Bytes pro Sekunde.

Das klingt nach einer kleinen Datenmenge, aber wenn Sie einen Mikrocontroller mit begrenztem Speicher verwenden, kann das viel sein. Zum Beispiel hat das Wio Terminal 192 KB Speicher, und dieser muss Programmcode und Variablen speichern. Selbst wenn Ihr Programmcode winzig w√§re, k√∂nnten Sie nicht mehr als 5 Sekunden Audio aufnehmen.

Mikrocontroller k√∂nnen auf zus√§tzlichen Speicher zugreifen, wie SD-Karten oder Flash-Speicher. Beim Bau eines IoT-Ger√§ts, das Audio aufnimmt, m√ºssen Sie sicherstellen, dass Sie nicht nur zus√§tzlichen Speicher haben, sondern dass Ihr Code das vom Mikrofon aufgenommene Audio direkt in diesen Speicher schreibt. Wenn Sie es in die Cloud senden, sollten Sie es aus dem Speicher streamen, anstatt den gesamten Audio-Datenblock auf einmal im Speicher zu halten. So vermeiden Sie, dass Ihnen der Speicher ausgeht.

## Audio von Ihrem IoT-Ger√§t aufnehmen

Ihr IoT-Ger√§t kann mit einem Mikrofon verbunden werden, um Audio aufzunehmen, das dann in Text umgewandelt werden kann. Es kann auch mit Lautsprechern verbunden werden, um Audio auszugeben. In sp√§teren Lektionen wird dies verwendet, um akustisches Feedback zu geben, aber es ist n√ºtzlich, Lautsprecher jetzt einzurichten, um das Mikrofon zu testen.

### Aufgabe ‚Äì Mikrofon und Lautsprecher konfigurieren

Arbeiten Sie die entsprechende Anleitung durch, um das Mikrofon und die Lautsprecher f√ºr Ihr IoT-Ger√§t zu konfigurieren:

* [Arduino ‚Äì Wio Terminal](wio-terminal-microphone.md)
* [Einplatinencomputer ‚Äì Raspberry Pi](pi-microphone.md)
* [Einplatinencomputer ‚Äì Virtuelles Ger√§t](virtual-device-microphone.md)

### Aufgabe ‚Äì Audio aufnehmen

Arbeiten Sie die entsprechende Anleitung durch, um Audio auf Ihrem IoT-Ger√§t aufzunehmen:

* [Arduino ‚Äì Wio Terminal](wio-terminal-audio.md)
* [Einplatinencomputer ‚Äì Raspberry Pi](pi-audio.md)
* [Einplatinencomputer ‚Äì Virtuelles Ger√§t](virtual-device-audio.md)

## Sprache in Text umwandeln

Sprache in Text, oder Spracherkennung, bedeutet, KI zu verwenden, um W√∂rter in einem Audiosignal in Text umzuwandeln.

### Sprachmodelle

Um Sprache in Text umzuwandeln, werden Abtastungen aus dem Audiosignal gruppiert und in ein maschinelles Lernmodell eingespeist, das auf einem Rekurrenten Neuronalen Netzwerk (RNN) basiert. Dies ist eine Art von maschinellem Lernmodell, das fr√ºhere Daten verwenden kann, um Entscheidungen √ºber eingehende Daten zu treffen. Zum Beispiel k√∂nnte das RNN einen Block von Audiodaten als den Klang ‚ÄûHel‚Äú erkennen und, wenn es einen weiteren Block erh√§lt, der wie ‚Äûlo‚Äú klingt, diese mit dem vorherigen Klang kombinieren, feststellen, dass ‚ÄûHello‚Äú ein g√ºltiges Wort ist, und dieses als Ergebnis ausw√§hlen.

ML-Modelle akzeptieren immer Daten derselben Gr√∂√üe. Der Bildklassifikator, den Sie in einer fr√ºheren Lektion erstellt haben, skaliert Bilder auf eine feste Gr√∂√üe und verarbeitet sie. Dasselbe gilt f√ºr Sprachmodelle: Sie m√ºssen Audiodaten in festen Gr√∂√üen verarbeiten. Sprachmodelle m√ºssen in der Lage sein, die Ergebnisse mehrerer Vorhersagen zu kombinieren, um die richtige Antwort zu finden, damit sie beispielsweise zwischen ‚ÄûHi‚Äú und ‚ÄûHighway‚Äú oder ‚Äûflock‚Äú und ‚Äûfloccinaucinihilipilification‚Äú unterscheiden k√∂nnen.

Sprachmodelle sind auch so fortschrittlich, dass sie den Kontext verstehen und die erkannten W√∂rter korrigieren k√∂nnen, w√§hrend mehr Daten verarbeitet werden. Wenn Sie beispielsweise sagen: ‚ÄûIch ging in die L√§den, um zwei Bananen und einen Apfel zu holen‚Äú, verwenden Sie drei W√∂rter, die gleich klingen, aber unterschiedlich geschrieben werden ‚Äì to, two und too. Sprachmodelle k√∂nnen den Kontext verstehen und die richtige Schreibweise des Wortes verwenden.
üíÅ Einige Sprachdienste erm√∂glichen Anpassungen, um besser in lauten Umgebungen wie Fabriken zu funktionieren oder mit branchenspezifischen Begriffen wie chemischen Namen umzugehen. Diese Anpassungen werden durch das Bereitstellen von Beispiel-Audio und einer Transkription trainiert und basieren auf Transferlernen ‚Äì √§hnlich wie Sie zuvor einen Bildklassifikator mit nur wenigen Bildern in einer fr√ºheren Lektion trainiert haben.
### Datenschutz

Beim Einsatz von Sprach-zu-Text in einem IoT-Ger√§t f√ºr Verbraucher ist Datenschutz von entscheidender Bedeutung. Diese Ger√§te h√∂ren kontinuierlich Audio, und als Verbraucher m√∂chten Sie nicht, dass alles, was Sie sagen, in die Cloud gesendet und in Text umgewandelt wird. Dies w√ºrde nicht nur viel Internetbandbreite verbrauchen, sondern auch erhebliche Datenschutzprobleme mit sich bringen, insbesondere wenn einige Hersteller von Smart-Ger√§ten zuf√§llig Audio ausw√§hlen, um [es von Menschen gegen den generierten Text zu validieren, um ihr Modell zu verbessern](https://www.theverge.com/2019/4/10/18305378/amazon-alexa-ai-voice-assistant-annotation-listen-private-recordings).

Sie m√∂chten, dass Ihr Smart-Ger√§t Audio nur dann zur Verarbeitung in die Cloud sendet, wenn Sie es verwenden, und nicht, wenn es Audio in Ihrem Zuhause h√∂rt, das private Besprechungen oder intime Interaktionen enthalten k√∂nnte. Die meisten Smart-Ger√§te funktionieren mit einem *Aktivierungswort*, einer Schl√ºsselphrase wie ‚ÄûAlexa‚Äú, ‚ÄûHey Siri‚Äú oder ‚ÄûOK Google‚Äú, die das Ger√§t dazu bringt, ‚Äûaufzuwachen‚Äú und zuzuh√∂ren, was Sie sagen, bis es eine Pause in Ihrer Sprache erkennt, die darauf hinweist, dass Sie aufgeh√∂rt haben, mit dem Ger√§t zu sprechen.

> üéì Die Erkennung von Aktivierungsw√∂rtern wird auch als *Keyword Spotting* oder *Keyword Recognition* bezeichnet.

Diese Aktivierungsw√∂rter werden auf dem Ger√§t und nicht in der Cloud erkannt. Diese Smart-Ger√§te verf√ºgen √ºber kleine KI-Modelle, die auf dem Ger√§t laufen und auf das Aktivierungswort h√∂ren. Wenn es erkannt wird, beginnt das Ger√§t, Audio zur Erkennung in die Cloud zu streamen. Diese Modelle sind sehr spezialisiert und h√∂ren nur auf das Aktivierungswort.

> üíÅ Einige Technologieunternehmen f√ºgen ihren Ger√§ten mehr Datenschutz hinzu und f√ºhren einen Teil der Sprach-zu-Text-Umwandlung direkt auf dem Ger√§t aus. Apple hat angek√ºndigt, dass sie im Rahmen ihrer Updates f√ºr iOS und macOS 2021 die Sprach-zu-Text-Umwandlung auf dem Ger√§t unterst√ºtzen werden und viele Anfragen ohne Nutzung der Cloud bearbeiten k√∂nnen. Dies ist dank leistungsstarker Prozessoren in ihren Ger√§ten m√∂glich, die ML-Modelle ausf√ºhren k√∂nnen.

‚úÖ Was denken Sie √ºber die Datenschutz- und ethischen Implikationen der Speicherung von Audio, das in die Cloud gesendet wird? Sollte dieses Audio gespeichert werden, und wenn ja, wie? Halten Sie die Nutzung von Aufnahmen durch Strafverfolgungsbeh√∂rden f√ºr einen guten Kompromiss im Hinblick auf den Verlust der Privatsph√§re?

Die Erkennung von Aktivierungsw√∂rtern verwendet normalerweise eine Technik namens TinyML, bei der ML-Modelle so konvertiert werden, dass sie auf Mikrocontrollern laufen k√∂nnen. Diese Modelle sind klein und verbrauchen sehr wenig Energie.

Um die Komplexit√§t des Trainings und der Nutzung eines Aktivierungswort-Modells zu vermeiden, wird der intelligente Timer, den Sie in dieser Lektion bauen, einen Knopf verwenden, um die Spracherkennung zu aktivieren.

> üíÅ Wenn Sie ein Modell zur Erkennung von Aktivierungsw√∂rtern erstellen m√∂chten, das auf dem Wio Terminal oder Raspberry Pi l√§uft, sehen Sie sich dieses [Tutorial zur Reaktion auf Ihre Stimme von Edge Impulse](https://docs.edgeimpulse.com/docs/responding-to-your-voice) an. Wenn Sie Ihren Computer daf√ºr nutzen m√∂chten, k√∂nnen Sie das [Schnellstart-Tutorial f√ºr benutzerdefinierte Schl√ºsselw√∂rter in den Microsoft-Dokumenten](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn) ausprobieren.

## Sprache in Text umwandeln

![Logo der Sprachdienste](../../../../../translated_images/azure-speech-logo.a1f08c4befb0159f2cb5d692d3baf5b599e7b44759d316da907bda1508f46a4a.de.png)

√Ñhnlich wie bei der Bildklassifikation in einem fr√ºheren Projekt gibt es vorgefertigte KI-Dienste, die Sprache als Audiodatei aufnehmen und in Text umwandeln k√∂nnen. Einer dieser Dienste ist der Speech Service, Teil der Cognitive Services, vorgefertigte KI-Dienste, die Sie in Ihren Apps verwenden k√∂nnen.

### Aufgabe - Konfigurieren einer Sprach-KI-Ressource

1. Erstellen Sie eine Ressourcengruppe f√ºr dieses Projekt mit dem Namen `smart-timer`.

1. Verwenden Sie den folgenden Befehl, um eine kostenlose Sprachressource zu erstellen:

    ```sh
    az cognitiveservices account create --name smart-timer \
                                        --resource-group smart-timer \
                                        --kind SpeechServices \
                                        --sku F0 \
                                        --yes \
                                        --location <location>
    ```

    Ersetzen Sie `<location>` durch den Standort, den Sie bei der Erstellung der Ressourcengruppe verwendet haben.

1. Sie ben√∂tigen einen API-Schl√ºssel, um von Ihrem Code aus auf die Sprachressource zuzugreifen. F√ºhren Sie den folgenden Befehl aus, um den Schl√ºssel zu erhalten:

    ```sh
    az cognitiveservices account keys list --name smart-timer \
                                           --resource-group smart-timer \
                                           --output table
    ```

    Kopieren Sie einen der Schl√ºssel.

### Aufgabe - Sprache in Text umwandeln

Arbeiten Sie die entsprechende Anleitung durch, um Sprache auf Ihrem IoT-Ger√§t in Text umzuwandeln:

* [Arduino - Wio Terminal](wio-terminal-speech-to-text.md)
* [Einplatinencomputer - Raspberry Pi](pi-speech-to-text.md)
* [Einplatinencomputer - Virtuelles Ger√§t](virtual-device-speech-to-text.md)

---

## üöÄ Herausforderung

Spracherkennung gibt es schon lange und sie verbessert sich kontinuierlich. Recherchieren Sie die aktuellen F√§higkeiten und vergleichen Sie, wie sich diese im Laufe der Zeit entwickelt haben, einschlie√ülich der Genauigkeit maschineller Transkriptionen im Vergleich zu menschlichen.

Was glauben Sie, h√§lt die Zukunft f√ºr die Spracherkennung bereit?

## Quiz nach der Vorlesung

[Quiz nach der Vorlesung](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/42)

## √úberpr√ºfung & Selbststudium

* Lesen Sie √ºber die verschiedenen Mikrofontypen und wie sie funktionieren im Artikel [Was ist der Unterschied zwischen dynamischen und Kondensatormikrofonen auf Musician's HQ](https://musicianshq.com/whats-the-difference-between-dynamic-and-condenser-microphones/).
* Lesen Sie mehr √ºber den Cognitive Services Speech Service in der [Dokumentation zum Speech Service auf Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/?WT.mc_id=academic-17441-jabenn).
* Lesen Sie √ºber Keyword Spotting in der [Dokumentation zur Keyword-Erkennung auf Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn).

## Aufgabe

[](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.