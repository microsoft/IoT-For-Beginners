<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "1c9e5fa8b7be726c75a97232b1e41c97",
  "translation_date": "2025-08-25T20:44:53+00:00",
  "source_file": "5-retail/lessons/2-check-stock-device/README.md",
  "language_code": "de"
}
-->
# Lagerbestand mit einem IoT-Ger√§t √ºberpr√ºfen

![Eine Sketchnote-√úbersicht dieser Lektion](../../../../../translated_images/lesson-20.0211df9551a8abb300fc8fcf7dc2789468dea2eabe9202273ac077b0ba37f15e.de.jpg)

> Sketchnote von [Nitya Narasimhan](https://github.com/nitya). Klicken Sie auf das Bild f√ºr eine gr√∂√üere Version.

## Quiz vor der Vorlesung

[Quiz vor der Vorlesung](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/39)

## Einf√ºhrung

In der vorherigen Lektion haben Sie die verschiedenen Einsatzm√∂glichkeiten der Objekterkennung im Einzelhandel kennengelernt. Au√üerdem haben Sie gelernt, wie man einen Objekterkenner trainiert, um Lagerbest√§nde zu identifizieren. In dieser Lektion erfahren Sie, wie Sie Ihren Objekterkenner von Ihrem IoT-Ger√§t aus nutzen k√∂nnen, um Lagerbest√§nde zu z√§hlen.

In dieser Lektion behandeln wir:

* [Lagerbestand z√§hlen](../../../../../5-retail/lessons/2-check-stock-device)
* [Den Objekterkenner von Ihrem IoT-Ger√§t aus aufrufen](../../../../../5-retail/lessons/2-check-stock-device)
* [Begrenzungsrahmen](../../../../../5-retail/lessons/2-check-stock-device)
* [Das Modell neu trainieren](../../../../../5-retail/lessons/2-check-stock-device)
* [Lagerbestand z√§hlen](../../../../../5-retail/lessons/2-check-stock-device)

> üóë Dies ist die letzte Lektion in diesem Projekt. Vergessen Sie nach Abschluss dieser Lektion und der Aufgabe nicht, Ihre Cloud-Dienste aufzur√§umen. Sie ben√∂tigen die Dienste, um die Aufgabe abzuschlie√üen, stellen Sie also sicher, dass Sie diese zuerst abschlie√üen.
>
> Konsultieren Sie bei Bedarf [die Anleitung zum Aufr√§umen Ihres Projekts](../../../clean-up.md) f√ºr Anweisungen, wie Sie dies tun k√∂nnen.

## Lagerbestand z√§hlen

Objekterkenner k√∂nnen f√ºr die Lagerbestandspr√ºfung verwendet werden, entweder um Lagerbest√§nde zu z√§hlen oder sicherzustellen, dass sich die Artikel dort befinden, wo sie sein sollten. IoT-Ger√§te mit Kameras k√∂nnen im gesamten Gesch√§ft eingesetzt werden, um Lagerbest√§nde zu √ºberwachen, beginnend mit Hotspots, an denen das Auff√ºllen von Artikeln wichtig ist, wie z. B. Bereiche, in denen kleine Mengen von hochpreisigen Artikeln gelagert werden.

Beispielsweise, wenn eine Kamera auf ein Regal zeigt, das 8 Dosen Tomatenmark aufnehmen kann, und ein Objekterkenner nur 7 Dosen erkennt, fehlt eine und muss nachgef√ºllt werden.

![7 Dosen Tomatenmark auf einem Regal, 4 in der oberen Reihe, 3 oben](../../../../../translated_images/stock-7-cans-tomato-paste.f86059cc573d7becaa89a0eafb9d2cd7e2fe37405a530fe565990e2333d0e4a1.de.png)

Im obigen Bild hat ein Objekterkenner 7 Dosen Tomatenmark auf einem Regal erkannt, das 8 Dosen aufnehmen kann. Das IoT-Ger√§t kann nicht nur eine Benachrichtigung √ºber die Notwendigkeit des Nachf√ºllens senden, sondern auch einen Hinweis auf den Standort des fehlenden Artikels geben, wichtige Daten, wenn Sie Roboter zum Auff√ºllen von Regalen verwenden.

> üíÅ Je nach Gesch√§ft und Beliebtheit des Artikels w√ºrde wahrscheinlich kein Nachf√ºllen erfolgen, wenn nur eine Dose fehlt. Sie m√ºssten einen Algorithmus entwickeln, der bestimmt, wann nachgef√ºllt werden soll, basierend auf Ihren Produkten, Kunden und anderen Kriterien.

‚úÖ In welchen anderen Szenarien k√∂nnten Sie Objekterkennung und Roboter kombinieren?

Manchmal kann das falsche Lager auf den Regalen sein. Dies k√∂nnte ein menschlicher Fehler beim Nachf√ºllen sein oder Kunden, die ihre Meinung √ºber einen Kauf √§ndern und einen Artikel in den n√§chstbesten freien Platz zur√ºcklegen. Wenn es sich um einen nicht verderblichen Artikel wie Konserven handelt, ist dies √§rgerlich. Wenn es sich um einen verderblichen Artikel wie Tiefk√ºhl- oder gek√ºhlte Waren handelt, kann dies bedeuten, dass das Produkt nicht mehr verkauft werden kann, da es m√∂glicherweise unm√∂glich ist, festzustellen, wie lange der Artikel au√üerhalb des Gefrierschranks war.

Die Objekterkennung kann verwendet werden, um unerwartete Artikel zu erkennen und erneut einen Menschen oder Roboter zu benachrichtigen, den Artikel so schnell wie m√∂glich zur√ºckzubringen.

![Eine fremde Dose Babymais auf dem Tomatenmark-Regal](../../../../../translated_images/stock-rogue-corn.be1f3ada8c4578544641af66671c1711a4c02297f14cc7f503354dae0d30a954.de.png)

Im obigen Bild wurde eine Dose Babymais neben das Tomatenmark-Regal gestellt. Der Objekterkenner hat dies erkannt, sodass das IoT-Ger√§t einen Menschen oder Roboter benachrichtigen kann, die Dose an ihren richtigen Platz zur√ºckzubringen.

## Den Objekterkenner von Ihrem IoT-Ger√§t aus aufrufen

Der Objekterkenner, den Sie in der letzten Lektion trainiert haben, kann von Ihrem IoT-Ger√§t aus aufgerufen werden.

### Aufgabe - Eine Iteration Ihres Objekterkenners ver√∂ffentlichen

Iterationen werden √ºber das Custom Vision-Portal ver√∂ffentlicht.

1. Starten Sie das Custom Vision-Portal unter [CustomVision.ai](https://customvision.ai) und melden Sie sich an, falls Sie es noch nicht ge√∂ffnet haben. √ñffnen Sie dann Ihr `stock-detector`-Projekt.

1. W√§hlen Sie die Registerkarte **Performance** aus den Optionen oben aus.

1. W√§hlen Sie die neueste Iteration aus der Liste *Iterations* auf der Seite aus.

1. W√§hlen Sie die Schaltfl√§che **Publish** f√ºr die Iteration aus.

    ![Die Schaltfl√§che Publish](../../../../../translated_images/custom-vision-object-detector-publish-button.34ee379fc650ccb9856c3868d0003f413b9529f102fc73c37168c98d721cc293.de.png)

1. Im Dialogfeld *Publish Model* setzen Sie die *Prediction resource* auf die `stock-detector-prediction`-Ressource, die Sie in der letzten Lektion erstellt haben. Lassen Sie den Namen als `Iteration2` und w√§hlen Sie die Schaltfl√§che **Publish**.

1. Sobald die Iteration ver√∂ffentlicht ist, w√§hlen Sie die Schaltfl√§che **Prediction URL**. Dies zeigt Details der Vorhersage-API, die Sie ben√∂tigen, um das Modell von Ihrem IoT-Ger√§t aus aufzurufen. Der untere Abschnitt ist mit *If you have an image file* beschriftet, und dies sind die Details, die Sie ben√∂tigen. Kopieren Sie die angezeigte URL, die etwa so aussieht:

    ```output
    https://<location>.api.cognitive.microsoft.com/customvision/v3.0/Prediction/<id>/detect/iterations/Iteration2/image
    ```

    Dabei ist `<location>` der Standort, den Sie beim Erstellen Ihrer Custom Vision-Ressource verwendet haben, und `<id>` eine lange ID aus Buchstaben und Zahlen.

    Kopieren Sie auch den Wert *Prediction-Key*. Dies ist ein sicherer Schl√ºssel, den Sie √ºbergeben m√ºssen, wenn Sie das Modell aufrufen. Nur Anwendungen, die diesen Schl√ºssel √ºbergeben, d√ºrfen das Modell verwenden, alle anderen Anwendungen werden abgelehnt.

    ![Das Dialogfeld der Vorhersage-API mit URL und Schl√ºssel](../../../../../translated_images/custom-vision-prediction-key-endpoint.30c569ffd0338864f319911f052d5e9b8c5066cb0800a26dd6f7ff5713130ad8.de.png)

‚úÖ Wenn eine neue Iteration ver√∂ffentlicht wird, hat sie einen anderen Namen. Wie w√ºrden Sie die Iteration √§ndern, die ein IoT-Ger√§t verwendet?

### Aufgabe - Den Objekterkenner von Ihrem IoT-Ger√§t aus aufrufen

Folgen Sie der entsprechenden Anleitung unten, um den Objekterkenner von Ihrem IoT-Ger√§t aus zu verwenden:

* [Arduino - Wio Terminal](wio-terminal-object-detector.md)
* [Einplatinencomputer - Raspberry Pi/virtuelles Ger√§t](single-board-computer-object-detector.md)

## Begrenzungsrahmen

Wenn Sie den Objekterkenner verwenden, erhalten Sie nicht nur die erkannten Objekte mit ihren Tags und Wahrscheinlichkeiten zur√ºck, sondern auch die Begrenzungsrahmen der Objekte. Diese definieren, wo der Objekterkenner das Objekt mit der angegebenen Wahrscheinlichkeit erkannt hat.

> üíÅ Ein Begrenzungsrahmen ist ein Rahmen, der den Bereich definiert, der das erkannte Objekt enth√§lt, ein Rahmen, der die Grenze f√ºr das Objekt definiert.

Die Ergebnisse einer Vorhersage im **Predictions**-Tab in Custom Vision haben die Begrenzungsrahmen, die auf das Bild gezeichnet sind, das zur Vorhersage gesendet wurde.

![4 Dosen Tomatenmark auf einem Regal mit Vorhersagen f√ºr die 4 Erkennungen von 35,8 %, 33,5 %, 25,7 % und 16,6 %](../../../../../translated_images/custom-vision-stock-prediction.942266ab1bcca3410ecdf23643b9f5f570cfab2345235074e24c51f285777613.de.png)

Im obigen Bild wurden 4 Dosen Tomatenmark erkannt. In den Ergebnissen wird ein rotes Quadrat f√ºr jedes Objekt, das im Bild erkannt wurde, √ºberlagert, das den Begrenzungsrahmen f√ºr das Bild angibt.

‚úÖ √ñffnen Sie die Vorhersagen in Custom Vision und sehen Sie sich die Begrenzungsrahmen an.

Begrenzungsrahmen werden mit 4 Werten definiert - oben, links, H√∂he und Breite. Diese Werte liegen auf einer Skala von 0-1 und repr√§sentieren die Positionen als Prozentsatz der Bildgr√∂√üe. Der Ursprung (die Position 0,0) ist die obere linke Ecke des Bildes, sodass der obere Wert die Entfernung von oben ist und die Unterseite des Begrenzungsrahmens die obere Position plus die H√∂he ist.

![Ein Begrenzungsrahmen um eine Dose Tomatenmark](../../../../../translated_images/bounding-box.1420a7ea0d3d15f71e1ffb5cf4b2271d184fac051f990abc541975168d163684.de.png)

Das obige Bild ist 600 Pixel breit und 800 Pixel hoch. Der Begrenzungsrahmen beginnt 320 Pixel nach unten, was eine obere Koordinate von 0,4 ergibt (800 x 0,4 = 320). Von links beginnt der Begrenzungsrahmen 240 Pixel quer, was eine linke Koordinate von 0,4 ergibt (600 x 0,4 = 240). Die H√∂he des Begrenzungsrahmens betr√§gt 240 Pixel, was einen H√∂henwert von 0,3 ergibt (800 x 0,3 = 240). Die Breite des Begrenzungsrahmens betr√§gt 120 Pixel, was einen Breitenwert von 0,2 ergibt (600 x 0,2 = 120).

| Koordinate | Wert  |
| ---------- | ----: |
| Oben       | 0,4   |
| Links      | 0,4   |
| H√∂he       | 0,3   |
| Breite     | 0,2   |

Die Verwendung von Prozentwerten von 0-1 bedeutet, dass unabh√§ngig davon, wie gro√ü das Bild skaliert wird, der Begrenzungsrahmen 0,4 des Weges entlang und nach unten beginnt und 0,3 der H√∂he und 0,2 der Breite betr√§gt.

Sie k√∂nnen Begrenzungsrahmen in Kombination mit Wahrscheinlichkeiten verwenden, um zu bewerten, wie genau eine Erkennung ist. Beispielsweise kann ein Objekterkenner mehrere Objekte erkennen, die sich √ºberlappen, z. B. eine Dose, die in einer anderen erkannt wird. Ihr Code k√∂nnte die Begrenzungsrahmen betrachten, verstehen, dass dies unm√∂glich ist, und alle Objekte ignorieren, die eine signifikante √úberlappung mit anderen Objekten haben.

![Zwei Begrenzungsrahmen √ºberlappen eine Dose Tomatenmark](../../../../../translated_images/overlap-object-detection.d431e03cae75072a2760430eca7f2c5fdd43045bfd72dadcbf12711f7cd6c2ae.de.png)

Im obigen Beispiel zeigt ein Begrenzungsrahmen eine vorhergesagte Dose Tomatenmark mit 78,3 %. Ein zweiter Begrenzungsrahmen ist etwas kleiner und befindet sich innerhalb des ersten Begrenzungsrahmens mit einer Wahrscheinlichkeit von 64,3 %. Ihr Code kann die Begrenzungsrahmen √ºberpr√ºfen, sehen, dass sie sich vollst√§ndig √ºberlappen, und die niedrigere Wahrscheinlichkeit ignorieren, da es unm√∂glich ist, dass eine Dose in einer anderen ist.

‚úÖ K√∂nnen Sie sich eine Situation vorstellen, in der es g√ºltig ist, ein Objekt innerhalb eines anderen zu erkennen?

## Das Modell neu trainieren

Wie beim Bildklassifikator k√∂nnen Sie Ihr Modell mit Daten, die von Ihrem IoT-Ger√§t erfasst wurden, neu trainieren. Die Verwendung dieser realen Daten stellt sicher, dass Ihr Modell gut funktioniert, wenn es von Ihrem IoT-Ger√§t aus verwendet wird.

Im Gegensatz zum Bildklassifikator k√∂nnen Sie nicht einfach ein Bild taggen. Stattdessen m√ºssen Sie jeden Begrenzungsrahmen √ºberpr√ºfen, der vom Modell erkannt wurde. Wenn der Rahmen um das falsche Objekt liegt, muss er gel√∂scht werden, wenn er sich an der falschen Stelle befindet, muss er angepasst werden.

### Aufgabe - Das Modell neu trainieren

1. Stellen Sie sicher, dass Sie eine Reihe von Bildern mit Ihrem IoT-Ger√§t erfasst haben.

1. W√§hlen Sie im **Predictions**-Tab ein Bild aus. Sie sehen rote Rahmen, die die Begrenzungsrahmen der erkannten Objekte anzeigen.

1. Arbeiten Sie jeden Begrenzungsrahmen durch. W√§hlen Sie ihn zuerst aus, und Sie sehen ein Pop-up, das das Tag anzeigt. Verwenden Sie die Griffe an den Ecken des Begrenzungsrahmens, um die Gr√∂√üe bei Bedarf anzupassen. Wenn das Tag falsch ist, entfernen Sie es mit der **X**-Schaltfl√§che und f√ºgen Sie das richtige Tag hinzu. Wenn der Begrenzungsrahmen kein Objekt enth√§lt, l√∂schen Sie ihn mit der Papierkorb-Schaltfl√§che.

1. Schlie√üen Sie den Editor, wenn Sie fertig sind, und das Bild wird vom **Predictions**-Tab in den **Training Images**-Tab verschoben. Wiederholen Sie den Vorgang f√ºr alle Vorhersagen.

1. Verwenden Sie die **Train**-Schaltfl√§che, um Ihr Modell neu zu trainieren. Sobald es trainiert wurde, ver√∂ffentlichen Sie die Iteration und aktualisieren Sie Ihr IoT-Ger√§t, um die URL der neuen Iteration zu verwenden.

1. Setzen Sie Ihren Code erneut ein und testen Sie Ihr IoT-Ger√§t.

## Lagerbestand z√§hlen

Mit einer Kombination aus der Anzahl der erkannten Objekte und den Begrenzungsrahmen k√∂nnen Sie den Lagerbestand auf einem Regal z√§hlen.

### Aufgabe - Lagerbestand z√§hlen

Folgen Sie der entsprechenden Anleitung unten, um den Lagerbestand mit den Ergebnissen des Objekterkenners von Ihrem IoT-Ger√§t aus zu z√§hlen:

* [Arduino - Wio Terminal](wio-terminal-count-stock.md)
* [Einplatinencomputer - Raspberry Pi/virtuelles Ger√§t](single-board-computer-count-stock.md)

---

## üöÄ Herausforderung

K√∂nnen Sie falschen Lagerbestand erkennen? Trainieren Sie Ihr Modell auf mehrere Objekte und aktualisieren Sie dann Ihre App, um Sie zu benachrichtigen, wenn falscher Lagerbestand erkannt wird.

Vielleicht k√∂nnen Sie dies sogar weiterf√ºhren und Lagerbestand nebeneinander auf demselben Regal erkennen und pr√ºfen, ob etwas an den falschen Platz gestellt wurde, indem Sie Grenzen f√ºr die Begrenzungsrahmen definieren.

## Quiz nach der Vorlesung

[Quiz nach der Vorlesung](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/40)

## √úberpr√ºfung & Selbststudium

* Erfahren Sie mehr dar√ºber, wie Sie ein End-to-End-System zur Lagererkennung entwerfen k√∂nnen, im [Out of stock detection at the edge pattern guide auf Microsoft Docs](https://docs.microsoft.com/hybrid/app-solutions/pattern-out-of-stock-at-edge?WT.mc_id=academic-17441-jabenn).
* Lernen Sie andere M√∂glichkeiten kennen, End-to-End-Einzelhandelsl√∂sungen zu erstellen, die eine Reihe von IoT- und Cloud-Diensten kombinieren, indem Sie dieses [Behind the scenes of a retail solution - Hands On! Video auf YouTube](https://www.youtube.com/watch?v=m3Pc300x2Mw) ansehen.

## Aufgabe

[Verwenden Sie Ihren Objekterkenner am Edge](assignment.md)

**Haftungsausschluss**:  
Dieses Dokument wurde mit dem KI-√úbersetzungsdienst [Co-op Translator](https://github.com/Azure/co-op-translator) √ºbersetzt. Obwohl wir uns um Genauigkeit bem√ºhen, beachten Sie bitte, dass automatisierte √úbersetzungen Fehler oder Ungenauigkeiten enthalten k√∂nnen. Das Originaldokument in seiner urspr√ºnglichen Sprache sollte als ma√ügebliche Quelle betrachtet werden. F√ºr kritische Informationen wird eine professionelle menschliche √úbersetzung empfohlen. Wir √ºbernehmen keine Haftung f√ºr Missverst√§ndnisse oder Fehlinterpretationen, die sich aus der Nutzung dieser √úbersetzung ergeben.