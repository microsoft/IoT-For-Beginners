<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6d6aa1be033625d201a190fc9c5cbfb4",
  "translation_date": "2025-08-27T21:01:28+00:00",
  "source_file": "6-consumer/lessons/1-speech-recognition/README.md",
  "language_code": "da"
}
-->
# Genkend tale med en IoT-enhed

![En sketchnote-oversigt over denne lektion](../../../../../translated_images/lesson-21.e34de51354d6606fb5ee08d8c89d0222eea0a2a7aaf744a8805ae847c4f69dc4.da.jpg)

> Sketchnote af [Nitya Narasimhan](https://github.com/nitya). Klik p√• billedet for en st√∏rre version.

Denne video giver en oversigt over Azure tale-tjenesten, et emne der vil blive d√¶kket i denne lektion:

[![S√•dan kommer du i gang med din Cognitive Services Speech-ressource fra Microsoft Azure YouTube-kanalen](https://img.youtube.com/vi/iW0Fw0l3mrA/0.jpg)](https://www.youtube.com/watch?v=iW0Fw0l3mrA)

> üé• Klik p√• billedet ovenfor for at se videoen

## Quiz f√∏r lektionen

[Quiz f√∏r lektionen](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/41)

## Introduktion

'Alexa, s√¶t en timer p√• 12 minutter'

'Alexa, status p√• timer'

'Alexa, s√¶t en timer p√• 8 minutter kaldet damp broccoli'

Smarte enheder bliver mere og mere udbredte. Ikke kun som smarte h√∏jttalere som HomePods, Echos og Google Homes, men ogs√• indbygget i vores telefoner, ure og endda lysarmaturer og termostater.

> üíÅ Jeg har mindst 19 enheder i mit hjem, der har stemmeassistenter, og det er bare dem, jeg kender til!

Stemmestyring √∏ger tilg√¶ngeligheden ved at give folk med begr√¶nset bev√¶gelighed mulighed for at interagere med enheder. Uanset om det er en permanent funktionsneds√¶ttelse som at v√¶re f√∏dt uden arme, en midlertidig skade som br√¶kkede arme, eller hvis man har h√¶nderne fulde af indk√∏b eller sm√• b√∏rn, kan det at styre vores hjem med stemmen i stedet for h√¶nderne √•bne op for en verden af muligheder. At r√•be 'Hey Siri, luk min garageport' mens man h√•ndterer en baby og en uregerlig tumling kan v√¶re en lille, men effektiv forbedring af hverdagen.

En af de mest popul√¶re anvendelser af stemmeassistenter er at s√¶tte timere, is√¶r k√∏kkentimere. At kunne s√¶tte flere timere med bare stemmen er en stor hj√¶lp i k√∏kkenet - ingen grund til at stoppe med at √¶lte dej, r√∏re i suppen eller rense h√¶nderne for dumplingsfyld for at bruge en fysisk timer.

I denne lektion vil du l√¶re om at integrere stemmegenkendelse i IoT-enheder. Du vil l√¶re om mikrofoner som sensorer, hvordan man optager lyd fra en mikrofon tilsluttet en IoT-enhed, og hvordan man bruger AI til at konvertere det, der h√∏res, til tekst. Gennem resten af dette projekt vil du bygge en smart k√∏kkentimer, der kan s√¶tte timere med din stemme p√• flere sprog.

I denne lektion vil vi d√¶kke:

* [Mikrofoner](../../../../../6-consumer/lessons/1-speech-recognition)
* [Optag lyd fra din IoT-enhed](../../../../../6-consumer/lessons/1-speech-recognition)
* [Tale til tekst](../../../../../6-consumer/lessons/1-speech-recognition)
* [Konverter tale til tekst](../../../../../6-consumer/lessons/1-speech-recognition)

## Mikrofoner

Mikrofoner er analoge sensorer, der konverterer lydb√∏lger til elektriske signaler. Vibrationer i luften f√•r komponenter i mikrofonen til at bev√¶ge sig en smule, hvilket skaber sm√• √¶ndringer i elektriske signaler. Disse √¶ndringer forst√¶rkes derefter for at generere et elektrisk output.

### Mikrofontyper

Mikrofoner findes i forskellige typer:

* Dynamisk - Dynamiske mikrofoner har en magnet, der er fastgjort til en bev√¶gelig membran, som bev√¶ger sig i en spole af ledning og skaber en elektrisk str√∏m. Dette er det modsatte af de fleste h√∏jttalere, der bruger en elektrisk str√∏m til at bev√¶ge en magnet i en spole af ledning, hvilket f√•r en membran til at skabe lyd. Det betyder, at h√∏jttalere kan bruges som dynamiske mikrofoner, og dynamiske mikrofoner kan bruges som h√∏jttalere. I enheder som intercoms, hvor en bruger enten lytter eller taler, men ikke begge dele, kan √©n enhed fungere som b√•de h√∏jttaler og mikrofon.

    Dynamiske mikrofoner beh√∏ver ikke str√∏m for at fungere; det elektriske signal skabes udelukkende af mikrofonen.

    ![Patti Smith synger i en Shure SM58 (dynamisk cardioid-type) mikrofon](../../../../../translated_images/dynamic-mic.8babac890a2d80dfb0874b5bf37d4b851fe2aeb9da6fd72945746176978bf3bb.da.jpg)

* B√•nd - B√•ndmikrofoner ligner dynamiske mikrofoner, men de har et metalb√•nd i stedet for en membran. Dette b√•nd bev√¶ger sig i et magnetfelt og genererer en elektrisk str√∏m. Ligesom dynamiske mikrofoner beh√∏ver b√•ndmikrofoner ikke str√∏m for at fungere.

    ![Edmund Lowe, amerikansk skuespiller, st√•r ved en radiomikrofon (m√¶rket for (NBC) Blue Network), holder manuskript, 1942](../../../../../translated_images/ribbon-mic.eacc8e092c7441caee6d7a81e2f40e1675bf36269848964c7c09c9a9acb05127.da.jpg)

* Kondensator - Kondensatormikrofoner har en tynd metalmembran og en fast metalbagplade. Elektricitet p√•f√∏res begge dele, og n√•r membranen vibrerer, √¶ndres den statiske ladning mellem pladerne og genererer et signal. Kondensatormikrofoner kr√¶ver str√∏m for at fungere - kaldet *Phantom power*.

    ![C451B sm√•-membran kondensatormikrofon fra AKG Acoustics](../../../../../translated_images/condenser-mic.6f6ed5b76ca19e0ec3fd0c544601542d4479a6cb7565db336de49fbbf69f623e.da.jpg)

* MEMS - Mikroelektromekaniske systemmikrofoner, eller MEMS, er mikrofoner p√• en chip. De har en trykf√∏lsom membran √¶tset p√• en siliciumchip og fungerer p√• samme m√•de som en kondensatormikrofon. Disse mikrofoner kan v√¶re meget sm√• og integreret i kredsl√∏b.

    ![En MEMS-mikrofon p√• et kredsl√∏b](../../../../../translated_images/mems-microphone.80574019e1f5e4d9ee72fed720ecd25a39fc2969c91355d17ebb24ba4159e4c4.da.png)

    P√• billedet ovenfor er chippen m√¶rket **LEFT** en MEMS-mikrofon med en lille membran mindre end en millimeter bred.

‚úÖ Unders√∏g: Hvilke mikrofoner har du omkring dig - enten i din computer, din telefon, dit headset eller i andre enheder? Hvilken type mikrofoner er det?

### Digital lyd

Lyd er et analogt signal, der b√¶rer meget detaljeret information. For at konvertere dette signal til digitalt skal lyden samples mange tusinde gange i sekundet.

> üéì Sampling er processen med at konvertere lydsignalet til en digital v√¶rdi, der repr√¶senterer signalet p√• det p√•g√¶ldende tidspunkt.

![Et linjediagram, der viser et signal med diskrete punkter p√• faste intervaller](../../../../../translated_images/sampling.6f4fadb3f2d9dfe7618f9edfe75a350e6b3f74293ec84f02ab69c19d2afe3d73.da.png)

Digital lyd samples ved hj√¶lp af Pulse Code Modulation, eller PCM. PCM indeb√¶rer at afl√¶se sp√¶ndingen af signalet og v√¶lge den n√¶rmeste diskrete v√¶rdi til den sp√¶nding ved hj√¶lp af en defineret st√∏rrelse.

> üíÅ Du kan t√¶nke p√• PCM som sensorversionen af pulsbreddemodulation, eller PWM (PWM blev d√¶kket tilbage i [lektion 3 af introduktionsprojektet](../../../1-getting-started/lessons/3-sensors-and-actuators/README.md#pulse-width-modulation)). PCM indeb√¶rer at konvertere et analogt signal til digitalt, PWM indeb√¶rer at konvertere et digitalt signal til analogt.

For eksempel tilbyder de fleste streamingtjenester 16-bit eller 24-bit lyd. Det betyder, at de konverterer sp√¶ndingen til en v√¶rdi, der passer ind i et 16-bit heltal eller 24-bit heltal. 16-bit lyd passer ind i en v√¶rdi, der sp√¶nder fra -32.768 til 32.767, 24-bit sp√¶nder fra ‚àí8.388.608 til 8.388.607. Jo flere bits, desto t√¶ttere er samplingen p√• det, vores √∏rer faktisk h√∏rer.

> üíÅ Du har m√•ske h√∏rt om 8-bit lyd, ofte kaldet LoFi. Dette er lyd samplet med kun 8 bits, alts√• -128 til 127. Den f√∏rste computerlyd var begr√¶nset til 8 bits p√• grund af hardwarebegr√¶nsninger, s√• dette ses ofte i retrospil.

Disse samples tages mange tusinde gange i sekundet ved hj√¶lp af veldefinerede samplingshastigheder m√•lt i KHz (tusind afl√¶sninger pr. sekund). Streamingtjenester bruger 48KHz til de fleste lydfiler, men nogle 'lossless' lydfiler bruger op til 96KHz eller endda 192KHz. Jo h√∏jere samplingshastighed, desto t√¶ttere er lyden p√• originalen, op til et punkt. Der er debat om, hvorvidt mennesker kan h√∏re forskel over 48KHz.

‚úÖ Unders√∏g: Hvis du bruger en streamingtjeneste, hvilken samplingshastighed og st√∏rrelse bruger den? Hvis du bruger CD'er, hvad er samplingshastigheden og st√∏rrelsen p√• CD-lyd?

Der findes en r√¶kke forskellige formater for lyddata. Du har sikkert h√∏rt om mp3-filer - lyddata, der er komprimeret for at g√∏re dem mindre uden at miste kvalitet. Ukomprimeret lyd gemmes ofte som en WAV-fil - dette er en fil med 44 bytes headerinformation efterfulgt af r√• lyddata. Headeren indeholder information som samplingshastighed (for eksempel 16000 for 16KHz) og sample-st√∏rrelse (16 for 16-bit) samt antallet af kanaler. Efter headeren indeholder WAV-filen de r√• lyddata.

> üéì Kanaler refererer til, hvor mange forskellige lydstr√∏mme der udg√∏r lyden. For eksempel, for stereo-lyd med venstre og h√∏jre, ville der v√¶re 2 kanaler. For 7.1 surround sound til et hjemmebiografsystem ville dette v√¶re 8.

### Lyddata st√∏rrelse

Lyddata er relativt store. For eksempel, optagelse af ukomprimeret 16-bit lyd ved 16KHz (en tilstr√¶kkelig hastighed til brug med tale-til-tekst-modeller) kr√¶ver 32KB data for hvert sekund af lyd:

* 16-bit betyder 2 bytes pr. sample (1 byte er 8 bits).
* 16KHz er 16.000 samples pr. sekund.
* 16.000 x 2 bytes = 32.000 bytes pr. sekund.

Dette lyder som en lille m√¶ngde data, men hvis du bruger en mikrocontroller med begr√¶nset hukommelse, kan det v√¶re meget. For eksempel har Wio Terminal 192KB hukommelse, og den skal opbevare programkode og variabler. Selv hvis din programkode var meget lille, kunne du ikke optage mere end 5 sekunder lyd.

Mikrocontrollere kan f√• adgang til ekstra lagerplads, s√•som SD-kort eller flash-hukommelse. N√•r du bygger en IoT-enhed, der optager lyd, skal du sikre dig, at du ikke kun har ekstra lagerplads, men ogs√• at din kode skriver den optagede lyd fra mikrofonen direkte til lageret. N√•r du sender det til skyen, skal du streame fra lageret til webanmodningen. P√• den m√•de undg√•r du at l√∏be t√∏r for hukommelse ved at fors√∏ge at holde hele lydblokken i hukommelsen p√• √©n gang.

## Optag lyd fra din IoT-enhed

Din IoT-enhed kan tilsluttes en mikrofon for at optage lyd, klar til konvertering til tekst. Den kan ogs√• tilsluttes h√∏jttalere for at afspille lyd. I senere lektioner vil dette blive brugt til at give lydfeedback, men det er nyttigt at ops√¶tte h√∏jttalere nu for at teste mikrofonen.

### Opgave - konfigurer din mikrofon og h√∏jttalere

F√∏lg den relevante vejledning for at konfigurere mikrofonen og h√∏jttalerne til din IoT-enhed:

* [Arduino - Wio Terminal](wio-terminal-microphone.md)
* [Single-board computer - Raspberry Pi](pi-microphone.md)
* [Single-board computer - Virtuel enhed](virtual-device-microphone.md)

### Opgave - optag lyd

F√∏lg den relevante vejledning for at optage lyd p√• din IoT-enhed:

* [Arduino - Wio Terminal](wio-terminal-audio.md)
* [Single-board computer - Raspberry Pi](pi-audio.md)
* [Single-board computer - Virtuel enhed](virtual-device-audio.md)

## Tale til tekst

Tale til tekst, eller talegenkendelse, indeb√¶rer brug af AI til at konvertere ord i et lydsignal til tekst.

### Talegenkendelsesmodeller

For at konvertere tale til tekst grupperes samples fra lydsignalet og f√∏res ind i en maskinl√¶ringsmodel baseret p√• et Recurrent Neural Network (RNN). Dette er en type maskinl√¶ringsmodel, der kan bruge tidligere data til at tr√¶ffe beslutninger om indkommende data. For eksempel kunne RNN'en registrere √©n blok af lydsamples som lyden 'Hel', og n√•r den modtager en anden blok, som den tror er lyden 'lo', kan den kombinere dette med den tidligere lyd, finde ud af at 'Hello' er et gyldigt ord og v√¶lge det som resultat.

ML-modeller accepterer altid data af samme st√∏rrelse hver gang. Billedklassifikatoren, du byggede i en tidligere lektion, √¶ndrer st√∏rrelsen p√• billeder til en fast st√∏rrelse og behandler dem. Det samme g√¶lder for tale-modeller; de skal behandle faste st√∏rrelser af lydstykker. Tale-modeller skal kunne kombinere output fra flere forudsigelser for at f√• svaret, s√• de kan skelne mellem 'Hi' og 'Highway' eller 'flock' og 'floccinaucinihilipilification'.

Tale-modeller er ogs√• avancerede nok til at forst√• kontekst og kan rette de ord, de registrerer, efterh√•nden som flere lyde behandles. For eksempel, hvis du siger "Jeg gik til butikkerne for at k√∏be to bananer og et √¶ble ogs√•", ville du bruge tre ord, der lyder ens, men staves forskelligt - to, to og too. Tale-modeller er i stand til at forst√• konteksten og bruge den korrekte stavem√•de af ordet.
üíÅ Nogle taletjenester tillader tilpasning, s√• de fungerer bedre i st√∏jende milj√∏er som fabrikker, eller med branchespecifikke ord som kemiske navne. Disse tilpasninger tr√¶nes ved at levere pr√∏veoptagelser og en transskription og fungerer ved hj√¶lp af transfer learning, p√• samme m√•de som du tidligere tr√¶nede en billedklassifikator med kun f√• billeder i en tidligere lektion.
### Privatliv

N√•r man bruger tale-til-tekst p√• en forbruger-IoT-enhed, er privatliv ekstremt vigtigt. Disse enheder lytter kontinuerligt til lyd, s√• som forbruger √∏nsker man ikke, at alt, hvad man siger, bliver sendt til skyen og konverteret til tekst. Ikke alene vil dette bruge en masse internetb√•ndbredde, det har ogs√• store privatlivsm√¶ssige konsekvenser, is√¶r n√•r nogle producenter af smarte enheder tilf√¶ldigt udv√¶lger lyd for [mennesker til at validere mod den genererede tekst for at forbedre deres model](https://www.theverge.com/2019/4/10/18305378/amazon-alexa-ai-voice-assistant-annotation-listen-private-recordings).

Man √∏nsker kun, at ens smarte enhed sender lyd til skyen til behandling, n√•r man bruger den, ikke n√•r den h√∏rer lyd i hjemmet, som kan inkludere private m√∏der eller intime interaktioner. De fleste smarte enheder fungerer ved hj√¶lp af et *v√•gningsord*, en n√∏glefrase som "Alexa", "Hey Siri" eller "OK Google", der f√•r enheden til at 'v√•gne op' og lytte til, hvad man siger, indtil den registrerer en pause i talen, hvilket indikerer, at man er f√¶rdig med at tale til enheden.

> üéì V√•gningsordsdetektion kaldes ogs√• *Keyword spotting* eller *Keyword recognition*.

Disse v√•gningsord registreres p√• enheden, ikke i skyen. Disse smarte enheder har sm√• AI-modeller, der k√∏rer p√• enheden og lytter efter v√•gningsordet, og n√•r det registreres, begynder de at streame lyden til skyen for genkendelse. Disse modeller er meget specialiserede og lytter kun efter v√•gningsordet.

> üíÅ Nogle teknologivirksomheder tilf√∏jer mere privatliv til deres enheder og udf√∏rer en del af tale-til-tekst-konverteringen p√• enheden. Apple har annonceret, at som en del af deres 2021 iOS- og macOS-opdateringer vil de underst√∏tte tale-til-tekst-konvertering p√• enheden og kunne h√•ndtere mange foresp√∏rgsler uden at skulle bruge skyen. Dette er muligt takket v√¶re kraftige processorer i deres enheder, der kan k√∏re ML-modeller.

‚úÖ Hvad mener du er de privatlivs- og etiske konsekvenser ved at gemme den lyd, der sendes til skyen? B√∏r denne lyd gemmes, og hvis ja, hvordan? Synes du, at brugen af optagelser til retsh√•ndh√¶velse er en god afvejning i forhold til tabet af privatliv?

V√•gningsordsdetektion bruger normalt en teknik kendt som TinyML, som er at konvertere ML-modeller til at kunne k√∏re p√• mikrocontrollere. Disse modeller er sm√• i st√∏rrelse og bruger meget lidt str√∏m til at k√∏re.

For at undg√• kompleksiteten ved at tr√¶ne og bruge en v√•gningsordsmodel vil den smarte timer, du bygger i denne lektion, bruge en knap til at aktivere talegenkendelsen.

> üíÅ Hvis du vil pr√∏ve at skabe en v√•gningsordsdetektionsmodel til at k√∏re p√• Wio Terminal eller Raspberry Pi, kan du tjekke denne [vejledning om at reagere p√• din stemme fra Edge Impulse](https://docs.edgeimpulse.com/docs/responding-to-your-voice). Hvis du vil bruge din computer til dette, kan du pr√∏ve [kom godt i gang med Custom Keyword quickstart p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn).

## Konverter tale til tekst

![Logo for taletjenester](../../../../../translated_images/azure-speech-logo.a1f08c4befb0159f2cb5d692d3baf5b599e7b44759d316da907bda1508f46a4a.da.png)

Ligesom med billedklassifikation i et tidligere projekt findes der forudbyggede AI-tjenester, der kan tage tale som en lydfil og konvertere det til tekst. En s√•dan tjeneste er Speech Service, en del af Cognitive Services, forudbyggede AI-tjenester, du kan bruge i dine apps.

### Opgave - konfigurer en tale-AI-ressource

1. Opret en Ressourcegruppe til dette projekt kaldet `smart-timer`.

1. Brug f√∏lgende kommando til at oprette en gratis taleressource:

    ```sh
    az cognitiveservices account create --name smart-timer \
                                        --resource-group smart-timer \
                                        --kind SpeechServices \
                                        --sku F0 \
                                        --yes \
                                        --location <location>
    ```

    Erstat `<location>` med den placering, du brugte, da du oprettede Ressourcegruppen.

1. Du skal bruge en API-n√∏gle for at f√• adgang til taleressourcen fra din kode. K√∏r f√∏lgende kommando for at f√• n√∏glen:

    ```sh
    az cognitiveservices account keys list --name smart-timer \
                                           --resource-group smart-timer \
                                           --output table
    ```

    Tag en kopi af en af n√∏glerne.

### Opgave - konverter tale til tekst

Gennemg√• den relevante vejledning for at konvertere tale til tekst p√• din IoT-enhed:

* [Arduino - Wio Terminal](wio-terminal-speech-to-text.md)
* [Single-board computer - Raspberry Pi](pi-speech-to-text.md)
* [Single-board computer - Virtuel enhed](virtual-device-speech-to-text.md)

---

## üöÄ Udfordring

Talegenkendelse har eksisteret i lang tid og forbedres kontinuerligt. Unders√∏g de nuv√¶rende muligheder og sammenlign, hvordan disse har udviklet sig over tid, herunder hvor pr√¶cise maskintransskriptioner er sammenlignet med menneskelige.

Hvad tror du, fremtiden bringer for talegenkendelse?

## Quiz efter lektionen

[Quiz efter lektionen](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/42)

## Gennemgang & Selvstudie

* L√¶s om de forskellige mikrofontyper, og hvordan de fungerer, i artiklen [hvad er forskellen mellem dynamiske og kondensatormikrofoner p√• Musician's HQ](https://musicianshq.com/whats-the-difference-between-dynamic-and-condenser-microphones/).
* L√¶s mere om Cognitive Services taletjeneste i [taletjenestens dokumentation p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/?WT.mc_id=academic-17441-jabenn).
* L√¶s om keyword spotting i [dokumentationen om keyword recognition p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn).

## Opgave

[](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj√¶lp af AI-overs√¶ttelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selvom vi bestr√¶ber os p√• at sikre n√∏jagtighed, skal det bem√¶rkes, at automatiserede overs√¶ttelser kan indeholde fejl eller un√∏jagtigheder. Det originale dokument p√• dets oprindelige sprog b√∏r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs√¶ttelse. Vi p√•tager os ikke ansvar for eventuelle misforst√•elser eller fejltolkninger, der m√•tte opst√• som f√∏lge af brugen af denne overs√¶ttelse.