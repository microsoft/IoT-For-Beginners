<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "7966848a1f870e4c42edb4db67b13c57",
  "translation_date": "2025-08-27T20:52:58+00:00",
  "source_file": "6-consumer/lessons/3-spoken-feedback/virtual-device-text-to-speech.md",
  "language_code": "no"
}
-->
# Tekst til tale - Virtuell IoT-enhet

I denne delen av leksjonen skal du skrive kode for 친 konvertere tekst til tale ved hjelp av tale-tjenesten.

## Konverter tekst til tale

SDK-en for tale-tjenester som du brukte i forrige leksjon for 친 konvertere tale til tekst, kan ogs친 brukes til 친 konvertere tekst tilbake til tale. N친r du ber om tale, m친 du spesifisere stemmen som skal brukes, ettersom tale kan genereres med en rekke forskjellige stemmer.

Hvert spr친k st칮tter et utvalg av ulike stemmer, og du kan f친 en liste over st칮ttede stemmer for hvert spr친k fra tale-tjenestens SDK.

### Oppgave - konverter tekst til tale

1. 칀pne `smart-timer`-prosjektet i VS Code, og s칮rg for at det virtuelle milj칮et er lastet inn i terminalen.

1. Importer `SpeechSynthesizer` fra pakken `azure.cognitiveservices.speech` ved 친 legge det til de eksisterende importene:

    ```python
    from azure.cognitiveservices.speech import SpeechConfig, SpeechRecognizer, SpeechSynthesizer
    ```

1. Over `say`-funksjonen, opprett en tale-konfigurasjon som skal brukes med tale-syntetisatoren:

    ```python
    speech_config = SpeechConfig(subscription=speech_api_key,
                                 region=location)
    speech_config.speech_synthesis_language = language
    speech_synthesizer = SpeechSynthesizer(speech_config=speech_config)
    ```

    Dette bruker samme API-n칮kkel, lokasjon og spr친k som ble brukt av gjenkjenneren.

1. Under dette, legg til f칮lgende kode for 친 hente en stemme og sette den p친 tale-konfigurasjonen:

    ```python
    voices = speech_synthesizer.get_voices_async().get().voices
    first_voice = next(x for x in voices if x.locale.lower() == language.lower())
    speech_config.speech_synthesis_voice_name = first_voice.short_name
    ```

    Dette henter en liste over alle tilgjengelige stemmer, og finner deretter den f칮rste stemmen som samsvarer med spr친ket som brukes.

    > 游누 Du kan f친 den fullstendige listen over st칮ttede stemmer fra [dokumentasjonen for spr친k- og stemmest칮tte p친 Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support?WT.mc_id=academic-17441-jabenn#text-to-speech). Hvis du 칮nsker 친 bruke en spesifikk stemme, kan du fjerne denne funksjonen og hardkode stemmen til stemmens navn fra denne dokumentasjonen. For eksempel:
    >
    > ```python
    > speech_config.speech_synthesis_voice_name = 'hi-IN-SwaraNeural'
    > ```

1. Oppdater innholdet i `say`-funksjonen for 친 generere SSML for responsen:

    ```python
    ssml =  f'<speak version=\'1.0\' xml:lang=\'{language}\'>'
    ssml += f'<voice xml:lang=\'{language}\' name=\'{first_voice.short_name}\'>'
    ssml += text
    ssml += '</voice>'
    ssml += '</speak>'
    ```

1. Under dette, stopp talegjenkjenningen, les opp SSML, og start deretter gjenkjenningen igjen:

    ```python
    recognizer.stop_continuous_recognition()
    speech_synthesizer.speak_ssml(ssml)
    recognizer.start_continuous_recognition()
    ```

    Gjenkjenningen stoppes mens teksten leses opp for 친 unng친 at kunngj칮ringen om at timeren starter blir oppfattet, sendt til LUIS og muligens tolket som en foresp칮rsel om 친 sette en ny timer.

    > 游누 Du kan teste dette ved 친 kommentere ut linjene som stopper og starter gjenkjenningen. Sett 칠n timer, og du kan oppleve at kunngj칮ringen setter en ny timer, som f칮rer til en ny kunngj칮ring, som igjen setter en ny timer, og s친 videre i det uendelige!

1. Kj칮r appen, og s칮rg for at funksjonsappen ogs친 kj칮rer. Sett noen timere, og du vil h칮re en muntlig respons som sier at timeren din er satt, og deretter en ny muntlig respons n친r timeren er ferdig.

> 游누 Du finner denne koden i [code-spoken-response/virtual-iot-device](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/virtual-iot-device)-mappen.

游 Timer-programmet ditt var en suksess!

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n칮yaktighet, v칝r oppmerksom p친 at automatiserte oversettelser kan inneholde feil eller un칮yaktigheter. Det originale dokumentet p친 sitt opprinnelige spr친k b칮r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst친elser eller feiltolkninger som oppst친r ved bruk av denne oversettelsen.