<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a202fa5889790a3777bfc33dd9f4b459",
  "translation_date": "2025-08-27T20:56:10+00:00",
  "source_file": "6-consumer/lessons/3-spoken-feedback/wio-terminal-text-to-speech.md",
  "language_code": "no"
}
-->
# Tekst til tale - Wio Terminal

I denne delen av leksjonen skal du konvertere tekst til tale for √• gi muntlig tilbakemelding.

## Tekst til tale

SDK-en for tale-tjenester som du brukte i forrige leksjon for √• konvertere tale til tekst, kan ogs√• brukes til √• konvertere tekst tilbake til tale.

## Hent en liste over stemmer

N√•r du ber om tale, m√• du spesifisere hvilken stemme som skal brukes, siden tale kan genereres med en rekke forskjellige stemmer. Hvert spr√•k st√∏tter et utvalg av ulike stemmer, og du kan f√• en liste over st√∏ttede stemmer for hvert spr√•k fra tale-tjenestenes SDK. Her kommer begrensningene til mikrokontrollere inn i bildet ‚Äì kallet for √• hente listen over stemmer som st√∏ttes av tekst-til-tale-tjenestene er et JSON-dokument p√• over 77KB, altfor stort til √• behandles av Wio Terminal. P√• tidspunktet for skriving inneholder hele listen 215 stemmer, hver definert av et JSON-dokument som dette:

```json
{
    "Name": "Microsoft Server Speech Text to Speech Voice (en-US, AriaNeural)",
    "DisplayName": "Aria",
    "LocalName": "Aria",
    "ShortName": "en-US-AriaNeural",
    "Gender": "Female",
    "Locale": "en-US",
    "StyleList": [
        "chat",
        "customerservice",
        "narration-professional",
        "newscast-casual",
        "newscast-formal",
        "cheerful",
        "empathetic"
    ],
    "SampleRateHertz": "24000",
    "VoiceType": "Neural",
    "Status": "GA"
}
```

Dette JSON-dokumentet er for stemmen **Aria**, som har flere stemmestiler. Alt som trengs for √• konvertere tekst til tale er kortnavnet, `en-US-AriaNeural`.

I stedet for √• laste ned og dekode hele denne listen p√• mikrokontrolleren, m√• du skrive litt mer serverl√∏s kode for √• hente listen over stemmer for spr√•ket du bruker, og kalle denne fra Wio Terminal. Koden din kan deretter velge en passende stemme fra listen, for eksempel den f√∏rste den finner.

### Oppgave - opprett en serverl√∏s funksjon for √• hente en liste over stemmer

1. √Öpne prosjektet ditt `smart-timer-trigger` i VS Code, og √•pne terminalen, og s√∏rg for at det virtuelle milj√∏et er aktivert. Hvis ikke, avslutt og opprett terminalen p√• nytt.

1. √Öpne filen `local.settings.json` og legg til innstillinger for tale-API-n√∏kkelen og plasseringen:

    ```json
    "SPEECH_KEY": "<key>",
    "SPEECH_LOCATION": "<location>"
    ```

    Erstatt `<key>` med API-n√∏kkelen for tale-tjenesten din. Erstatt `<location>` med plasseringen du brukte da du opprettet tale-tjenesten.

1. Legg til en ny HTTP-trigger i denne appen kalt `get-voices` ved √• bruke f√∏lgende kommando fra terminalen i rotmappen til funksjonsapp-prosjektet:

    ```sh
    func new --name get-voices --template "HTTP trigger"
    ```

    Dette vil opprette en HTTP-trigger kalt `get-voices`.

1. Erstatt innholdet i filen `__init__.py` i mappen `get-voices` med f√∏lgende:

    ```python
    import json
    import os
    import requests
    
    import azure.functions as func
    
    def main(req: func.HttpRequest) -> func.HttpResponse:
        location = os.environ['SPEECH_LOCATION']
        speech_key = os.environ['SPEECH_KEY']
    
        req_body = req.get_json()
        language = req_body['language']
    
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/voices/list'
    
        headers = {
            'Ocp-Apim-Subscription-Key': speech_key
        }
    
        response = requests.get(url, headers=headers)
        voices_json = json.loads(response.text)
    
        voices = filter(lambda x: x['Locale'].lower() == language.lower(), voices_json)
        voices = map(lambda x: x['ShortName'], voices)
    
        return func.HttpResponse(json.dumps(list(voices)), status_code=200)
    ```

    Denne koden gj√∏r et HTTP-kall til endepunktet for √• hente stemmene. Listen over stemmer er en stor JSON-blokk med stemmer for alle spr√•k, s√• stemmene for spr√•ket som sendes i foresp√∏rselens kropp blir filtrert ut, og deretter blir kortnavnet hentet ut og returnert som en JSON-liste. Kortnavnet er verdien som trengs for √• konvertere tekst til tale, s√• bare denne verdien returneres.

    > üíÅ Du kan endre filteret etter behov for √• velge bare de stemmene du √∏nsker.

    Dette reduserer datast√∏rrelsen fra 77KB (p√• tidspunktet for skriving) til et mye mindre JSON-dokument. For eksempel, for amerikanske stemmer er dette 408 byte.

1. Kj√∏r funksjonsappen din lokalt. Du kan deretter kalle denne ved √• bruke et verkt√∏y som curl p√• samme m√•te som du testet HTTP-triggeren `text-to-timer`. S√∏rg for √• sende spr√•ket som en JSON-kropp:

    ```json
    {
        "language":"<language>"
    }
    ```

    Erstatt `<language>` med spr√•ket ditt, for eksempel `en-GB` eller `zh-CN`.

> üíÅ Du finner denne koden i mappen [code-spoken-response/functions](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/functions).

### Oppgave - hent stemmen fra Wio Terminal

1. √Öpne prosjektet `smart-timer` i VS Code hvis det ikke allerede er √•pent.

1. √Öpne header-filen `config.h` og legg til URL-en for funksjonsappen din:

    ```cpp
    const char *GET_VOICES_FUNCTION_URL = "<URL>";
    ```

    Erstatt `<URL>` med URL-en for HTTP-triggeren `get-voices` i funksjonsappen din. Dette vil v√¶re det samme som verdien for `TEXT_TO_TIMER_FUNCTION_URL`, bortsett fra at funksjonsnavnet er `get-voices` i stedet for `text-to-timer`.

1. Opprett en ny fil i mappen `src` kalt `text_to_speech.h`. Denne vil bli brukt til √• definere en klasse for √• konvertere fra tekst til tale.

1. Legg til f√∏lgende include-direktiver √∏verst i den nye filen `text_to_speech.h`:

    ```cpp
    #pragma once

    #include <Arduino.h>
    #include <ArduinoJson.h>
    #include <HTTPClient.h>
    #include <Seeed_FS.h>
    #include <SD/Seeed_SD.h>
    #include <WiFiClient.h>
    #include <WiFiClientSecure.h>
    
    #include "config.h"
    #include "speech_to_text.h"
    ```

1. Legg til f√∏lgende kode under dette for √• erkl√¶re klassen `TextToSpeech`, sammen med en instans som kan brukes i resten av applikasjonen:

    ```cpp
    class TextToSpeech
    {
    public:
    private:
    };
    
    TextToSpeech textToSpeech;
    ```

1. For √• kalle funksjonsappen din, m√• du erkl√¶re en WiFi-klient. Legg til f√∏lgende i den private delen av klassen:

    ```cpp
    WiFiClient _client;
    ```

1. I den private delen, legg til et felt for den valgte stemmen:

    ```cpp
    String _voice;
    ```

1. I den offentlige delen, legg til en `init`-funksjon som vil hente den f√∏rste stemmen:

    ```cpp
    void init()
    {
    }
    ```

1. For √• hente stemmene, m√• et JSON-dokument sendes til funksjonsappen med spr√•ket. Legg til f√∏lgende kode i `init`-funksjonen for √• opprette dette JSON-dokumentet:

    ```cpp
    DynamicJsonDocument doc(1024);
    doc["language"] = LANGUAGE;

    String body;
    serializeJson(doc, body);
    ```

1. Opprett deretter en `HTTPClient`, og bruk den til √• kalle funksjonsappen for √• hente stemmene, ved √• sende JSON-dokumentet:

    ```cpp
    HTTPClient httpClient;
    httpClient.begin(_client, GET_VOICES_FUNCTION_URL);

    int httpResponseCode = httpClient.POST(body);
    ```

1. Under dette, legg til kode for √• sjekke svarskoden, og hvis den er 200 (suksess), hent listen over stemmer og velg den f√∏rste fra listen:

    ```cpp
    if (httpResponseCode == 200)
    {
        String result = httpClient.getString();
        Serial.println(result);

        DynamicJsonDocument doc(1024);
        deserializeJson(doc, result.c_str());

        JsonArray obj = doc.as<JsonArray>();
        _voice = obj[0].as<String>();

        Serial.print("Using voice ");
        Serial.println(_voice);
    }
    else
    {
        Serial.print("Failed to get voices - error ");
        Serial.println(httpResponseCode);
    }
    ```

1. Etter dette, avslutt HTTP-klienttilkoblingen:

    ```cpp
    httpClient.end();
    ```

1. √Öpne filen `main.cpp`, og legg til f√∏lgende include-direktiv √∏verst for √• inkludere denne nye header-filen:

    ```cpp
    #include "text_to_speech.h"
    ```

1. I `setup`-funksjonen, under kallet til `speechToText.init();`, legg til f√∏lgende for √• initialisere klassen `TextToSpeech`:

    ```cpp
    textToSpeech.init();
    ```

1. Bygg denne koden, last den opp til Wio Terminal og test den gjennom seriemonitoren. S√∏rg for at funksjonsappen din kj√∏rer.

    Du vil se listen over tilgjengelige stemmer returnert fra funksjonsappen, sammen med den valgte stemmen.

    ```output
    --- Available filters and text transformations: colorize, debug, default, direct, hexlify, log2file, nocontrol, printable, send_on_enter, time
    --- More details at http://bit.ly/pio-monitor-filters
    --- Miniterm on /dev/cu.usbmodem1101  9600,8,N,1 ---
    --- Quit: Ctrl+C | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---
    Connecting to WiFi..
    Connected!
    Got access token.
    ["en-US-JennyNeural", "en-US-JennyMultilingualNeural", "en-US-GuyNeural", "en-US-AriaNeural", "en-US-AmberNeural", "en-US-AnaNeural", "en-US-AshleyNeural", "en-US-BrandonNeural", "en-US-ChristopherNeural", "en-US-CoraNeural", "en-US-ElizabethNeural", "en-US-EricNeural", "en-US-JacobNeural", "en-US-MichelleNeural", "en-US-MonicaNeural", "en-US-AriaRUS", "en-US-BenjaminRUS", "en-US-GuyRUS", "en-US-ZiraRUS"]
    Using voice en-US-JennyNeural
    Ready.
    ```

## Konverter tekst til tale

N√•r du har en stemme √• bruke, kan den brukes til √• konvertere tekst til tale. De samme minnebegrensningene med stemmer gjelder ogs√• n√•r du konverterer tekst til tale, s√• du m√• skrive talen til et SD-kort for √• kunne spille den av via ReSpeaker.

> üíÅ I tidligere leksjoner i dette prosjektet brukte du flashminne for √• lagre tale fanget fra mikrofonen. Denne leksjonen bruker et SD-kort, da det er enklere √• spille av lyd fra det ved hjelp av Seeed-lydbibliotekene.

Det er ogs√• en annen begrensning √• vurdere, nemlig tilgjengelige lyddata fra tale-tjenesten og formatene som Wio Terminal st√∏tter. I motsetning til fullverdige datamaskiner, kan lydbiblioteker for mikrokontrollere v√¶re sv√¶rt begrenset i lydformatene de st√∏tter. For eksempel st√∏tter Seeed Arduino Audio-biblioteket som kan spille lyd via ReSpeaker kun lyd med en samplingsfrekvens p√• 44,1 kHz. Azure tale-tjenester kan levere lyd i flere formater, men ingen av dem bruker denne samplingsfrekvensen; de tilbyr kun 8 kHz, 16 kHz, 24 kHz og 48 kHz. Dette betyr at lyden m√• re-samples til 44,1 kHz, noe som krever mer ressurser enn det Wio Terminal har, spesielt minne.

N√•r det er behov for √• manipulere data som dette, er det ofte bedre √• bruke serverl√∏s kode, spesielt hvis dataene hentes via et webkall. Wio Terminal kan kalle en serverl√∏s funksjon, sende inn teksten som skal konverteres, og den serverl√∏se funksjonen kan b√•de kalle tale-tjenesten for √• konvertere tekst til tale, samt re-sample lyden til den n√∏dvendige samplingsfrekvensen. Den kan deretter returnere lyden i det formatet Wio Terminal trenger for √• lagres p√• SD-kortet og spilles av via ReSpeaker.

### Oppgave - opprett en serverl√∏s funksjon for √• konvertere tekst til tale

1. √Öpne prosjektet ditt `smart-timer-trigger` i VS Code, og √•pne terminalen, og s√∏rg for at det virtuelle milj√∏et er aktivert. Hvis ikke, avslutt og opprett terminalen p√• nytt.

1. Legg til en ny HTTP-trigger i denne appen kalt `text-to-speech` ved √• bruke f√∏lgende kommando fra terminalen i rotmappen til funksjonsapp-prosjektet:

    ```sh
    func new --name text-to-speech --template "HTTP trigger"
    ```

    Dette vil opprette en HTTP-trigger kalt `text-to-speech`.

1. Pip-pakken [librosa](https://librosa.org) har funksjoner for √• re-sample lyd, s√• legg denne til i filen `requirements.txt`:

    ```sh
    librosa
    ```

    N√•r dette er lagt til, installer Pip-pakkene ved √• bruke f√∏lgende kommando fra VS Code-terminalen:

    ```sh
    pip install -r requirements.txt
    ```

    > ‚ö†Ô∏è Hvis du bruker Linux, inkludert Raspberry Pi OS, m√• du kanskje installere `libsndfile` med f√∏lgende kommando:
    >
    > ```sh
    > sudo apt update
    > sudo apt install libsndfile1-dev
    > ```

1. For √• konvertere tekst til tale kan du ikke bruke tale-API-n√∏kkelen direkte, i stedet m√• du be om en tilgangstoken, ved √• bruke API-n√∏kkelen for √• autentisere foresp√∏rselen om tilgangstoken. √Öpne filen `__init__.py` fra mappen `text-to-speech` og erstatt all koden i den med f√∏lgende:

    ```python
    import io
    import os
    import requests
    
    import librosa
    import soundfile as sf
    import azure.functions as func
    
    location = os.environ['SPEECH_LOCATION']
    speech_key = os.environ['SPEECH_KEY']
    
    def get_access_token():
        headers = {
            'Ocp-Apim-Subscription-Key': speech_key
        }
    
        token_endpoint = f'https://{location}.api.cognitive.microsoft.com/sts/v1.0/issuetoken'
        response = requests.post(token_endpoint, headers=headers)
        return str(response.text)
    ```

    Dette definerer konstanter for plasseringen og tale-n√∏kkelen som vil bli lest fra innstillingene. Det definerer deretter funksjonen `get_access_token` som vil hente en tilgangstoken for tale-tjenesten.

1. Under denne koden, legg til f√∏lgende:

    ```python
    playback_format = 'riff-48khz-16bit-mono-pcm'

    def main(req: func.HttpRequest) -> func.HttpResponse:
        req_body = req.get_json()
        language = req_body['language']
        voice = req_body['voice']
        text = req_body['text']
    
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/v1'
    
        headers = {
            'Authorization': 'Bearer ' + get_access_token(),
            'Content-Type': 'application/ssml+xml',
            'X-Microsoft-OutputFormat': playback_format
        }
    
        ssml =  f'<speak version=\'1.0\' xml:lang=\'{language}\'>'
        ssml += f'<voice xml:lang=\'{language}\' name=\'{voice}\'>'
        ssml += text
        ssml += '</voice>'
        ssml += '</speak>'
    
        response = requests.post(url, headers=headers, data=ssml.encode('utf-8'))
    
        raw_audio, sample_rate = librosa.load(io.BytesIO(response.content), sr=48000)
        resampled = librosa.resample(raw_audio, sample_rate, 44100)
        
        output_buffer = io.BytesIO()
        sf.write(output_buffer, resampled, 44100, 'PCM_16', format='wav')
        output_buffer.seek(0)
    
        return func.HttpResponse(output_buffer.read(), status_code=200)
    ```

    Dette definerer HTTP-triggeren som konverterer tekst til tale. Den henter teksten som skal konverteres, spr√•ket og stemmen fra JSON-kroppen sendt til foresp√∏rselen, bygger litt SSML for √• be om talen, og kaller deretter den relevante REST-API-en ved √• autentisere med tilgangstokenet. Dette REST-API-kallet returnerer lyden kodet som en 16-bit, 48 kHz mono WAV-fil, definert av verdien av `playback_format`, som sendes til REST-API-kallet.

    Dette blir deretter re-samplet av `librosa` fra en samplingsfrekvens p√• 48 kHz til en samplingsfrekvens p√• 44,1 kHz, og denne lyden lagres deretter i en bin√¶r buffer som returneres.

1. Kj√∏r funksjonsappen din lokalt, eller distribuer den til skyen. Du kan deretter kalle denne ved √• bruke et verkt√∏y som curl p√• samme m√•te som du testet HTTP-triggeren `text-to-timer`. S√∏rg for √• sende spr√•ket, stemmen og teksten som JSON-kropp:

    ```json
    {
        "language": "<language>",
        "voice": "<voice>",
        "text": "<text>"
    }
    ```

    Erstatt `<language>` med spr√•ket ditt, for eksempel `en-GB` eller `zh-CN`. Erstatt `<voice>` med stemmen du vil bruke. Erstatt `<text>` med teksten du vil konvertere til tale. Du kan lagre utdataene til en fil og spille den av med en hvilken som helst lydspiller som kan spille WAV-filer.

    For eksempel, for √• konvertere "Hello" til tale ved bruk av amerikansk engelsk med stemmen Jenny Neural, med funksjonsappen kj√∏rende lokalt, kan du bruke f√∏lgende curl-kommando:

    ```sh
    curl -X GET 'http://localhost:7071/api/text-to-speech' \
         -H 'Content-Type: application/json' \
         -o hello.wav \
         -d '{
           "language":"en-US",
           "voice": "en-US-JennyNeural",
           "text": "Hello"
         }'
    ```

    Dette vil lagre lyden til `hello.wav` i den gjeldende katalogen.

> üíÅ Du finner denne koden i mappen [code-spoken-response/functions](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/functions).

### Oppgave - hent talen fra Wio Terminal

1. √Öpne prosjektet `smart-timer` i VS Code hvis det ikke allerede er √•pent.

1. √Öpne header-filen `config.h` og legg til URL-en for funksjonsappen din:

    ```cpp
    const char *TEXT_TO_SPEECH_FUNCTION_URL = "<URL>";
    ```

    Erstatt `<URL>` med URL-en for HTTP-triggeren `text-to-speech` i funksjonsappen din. Dette vil v√¶re det samme som verdien for `TEXT_TO_TIMER_FUNCTION_URL`, bortsett fra at funksjonsnavnet er `text-to-speech` i stedet for `text-to-timer`.

1. √Öpne header-filen `text_to_speech.h`, og legg til f√∏lgende metode i den offentlige delen av klassen `TextToSpeech`:

    ```cpp
    void convertTextToSpeech(String text)
    {
    }
    ```

1. I metoden `convertTextToSpeech`, legg til f√∏lgende kode for √• opprette JSON som skal sendes til funksjonsappen:

    ```cpp
    DynamicJsonDocument doc(1024);
    doc["language"] = LANGUAGE;
    doc["voice"] = _voice;
    doc["text"] = text;

    String body;
    serializeJson(doc, body);
    ```

    Dette skriver spr√•ket, stemmen og teksten til JSON-dokumentet, og serialiserer det deretter til en streng.

1. Under dette, legg til f√∏lgende kode for √• kalle funksjonsappen:

    ```cpp
    HTTPClient httpClient;
    httpClient.begin(_client, TEXT_TO_SPEECH_FUNCTION_URL);

    int httpResponseCode = httpClient.POST(body);
    ```

    Dette oppretter en HTTPClient, og gj√∏r deretter en POST-foresp√∏rsel med JSON-dokumentet til HTTP-triggeren for tekst til tale.

1. Hvis kallet fungerer, kan de r√• bin√¶re dataene som returneres fra funksjonsapp-kallet streames til en fil p√• SD-kortet. Legg til f√∏lgende kode for √• gj√∏re dette:

    ```cpp
    if (httpResponseCode == 200)
    {            
        File wav_file = SD.open("SPEECH.WAV", FILE_WRITE);
        httpClient.writeToStream(&wav_file);
        wav_file.close();
    }
    else
    {
        Serial.print("Failed to get speech - error ");
        Serial.println(httpResponseCode);
    }
    ```

    Denne koden sjekker svaret, og hvis det er 200 (suksess), streames de bin√¶re dataene til en fil i roten av SD-kortet kalt `SPEECH.WAV`.

1. P√• slutten av denne metoden, lukk HTTP-tilkoblingen:

    ```cpp
    httpClient.end();
    ```

1. Teksten som skal sies kan n√• konverteres til lyd. I filen `main.cpp`, legg til f√∏lgende linje p√• slutten av funksjonen `say` for √• konvertere teksten som skal sies til lyd:
```cpp
    textToSpeech.convertTextToSpeech(text);
    ```

### Oppgave - spille av lyd fra din Wio Terminal

**Kommer snart**

## Distribuere funksjonsappen din til skyen

Grunnen til at funksjonsappen kj√∏res lokalt er fordi `librosa` Pip-pakken p√• Linux har en avhengighet til et bibliotek som ikke er installert som standard, og dette m√• installeres f√∏r funksjonsappen kan kj√∏re. Funksjonsapper er serverl√∏se - det finnes ingen servere du kan administrere selv, s√• det er ingen m√•te √• installere dette biblioteket p√• forh√•nd.

L√∏sningen er √• distribuere funksjonsappen din ved hjelp av en Docker-container. Denne containeren distribueres av skyen n√•r det er behov for √• starte en ny instans av funksjonsappen din (for eksempel n√•r ettersp√∏rselen overstiger tilgjengelige ressurser, eller hvis funksjonsappen ikke har v√¶rt brukt p√• en stund og er stengt ned).

Du finner instruksjoner for √• sette opp en funksjonsapp og distribuere via Docker i [dokumentasjonen for √• opprette en funksjon p√• Linux ved hjelp av en tilpasset container p√• Microsoft Docs](https://docs.microsoft.com/azure/azure-functions/functions-create-function-linux-custom-image?WT.mc_id=academic-17441-jabenn&tabs=bash%2Cazurecli&pivots=programming-language-python).

N√•r dette er distribuert, kan du overf√∏re koden for din Wio Terminal for √• f√• tilgang til denne funksjonen:

1. Legg til Azure Functions-sertifikatet i `config.h`:

    ```cpp
    const char *FUNCTIONS_CERTIFICATE =
        "-----BEGIN CERTIFICATE-----\r\n"
        "MIIFWjCCBEKgAwIBAgIQDxSWXyAgaZlP1ceseIlB4jANBgkqhkiG9w0BAQsFADBa\r\n"
        "MQswCQYDVQQGEwJJRTESMBAGA1UEChMJQmFsdGltb3JlMRMwEQYDVQQLEwpDeWJl\r\n"
        "clRydXN0MSIwIAYDVQQDExlCYWx0aW1vcmUgQ3liZXJUcnVzdCBSb290MB4XDTIw\r\n"
        "MDcyMTIzMDAwMFoXDTI0MTAwODA3MDAwMFowTzELMAkGA1UEBhMCVVMxHjAcBgNV\r\n"
        "BAoTFU1pY3Jvc29mdCBDb3Jwb3JhdGlvbjEgMB4GA1UEAxMXTWljcm9zb2Z0IFJT\r\n"
        "QSBUTFMgQ0EgMDEwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQCqYnfP\r\n"
        "mmOyBoTzkDb0mfMUUavqlQo7Rgb9EUEf/lsGWMk4bgj8T0RIzTqk970eouKVuL5R\r\n"
        "IMW/snBjXXgMQ8ApzWRJCZbar879BV8rKpHoAW4uGJssnNABf2n17j9TiFy6BWy+\r\n"
        "IhVnFILyLNK+W2M3zK9gheiWa2uACKhuvgCca5Vw/OQYErEdG7LBEzFnMzTmJcli\r\n"
        "W1iCdXby/vI/OxbfqkKD4zJtm45DJvC9Dh+hpzqvLMiK5uo/+aXSJY+SqhoIEpz+\r\n"
        "rErHw+uAlKuHFtEjSeeku8eR3+Z5ND9BSqc6JtLqb0bjOHPm5dSRrgt4nnil75bj\r\n"
        "c9j3lWXpBb9PXP9Sp/nPCK+nTQmZwHGjUnqlO9ebAVQD47ZisFonnDAmjrZNVqEX\r\n"
        "F3p7laEHrFMxttYuD81BdOzxAbL9Rb/8MeFGQjE2Qx65qgVfhH+RsYuuD9dUw/3w\r\n"
        "ZAhq05yO6nk07AM9c+AbNtRoEcdZcLCHfMDcbkXKNs5DJncCqXAN6LhXVERCw/us\r\n"
        "G2MmCMLSIx9/kwt8bwhUmitOXc6fpT7SmFvRAtvxg84wUkg4Y/Gx++0j0z6StSeN\r\n"
        "0EJz150jaHG6WV4HUqaWTb98Tm90IgXAU4AW2GBOlzFPiU5IY9jt+eXC2Q6yC/Zp\r\n"
        "TL1LAcnL3Qa/OgLrHN0wiw1KFGD51WRPQ0Sh7QIDAQABo4IBJTCCASEwHQYDVR0O\r\n"
        "BBYEFLV2DDARzseSQk1Mx1wsyKkM6AtkMB8GA1UdIwQYMBaAFOWdWTCCR1jMrPoI\r\n"
        "VDaGezq1BE3wMA4GA1UdDwEB/wQEAwIBhjAdBgNVHSUEFjAUBggrBgEFBQcDAQYI\r\n"
        "KwYBBQUHAwIwEgYDVR0TAQH/BAgwBgEB/wIBADA0BggrBgEFBQcBAQQoMCYwJAYI\r\n"
        "KwYBBQUHMAGGGGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTA6BgNVHR8EMzAxMC+g\r\n"
        "LaArhilodHRwOi8vY3JsMy5kaWdpY2VydC5jb20vT21uaXJvb3QyMDI1LmNybDAq\r\n"
        "BgNVHSAEIzAhMAgGBmeBDAECATAIBgZngQwBAgIwCwYJKwYBBAGCNyoBMA0GCSqG\r\n"
        "SIb3DQEBCwUAA4IBAQCfK76SZ1vae4qt6P+dTQUO7bYNFUHR5hXcA2D59CJWnEj5\r\n"
        "na7aKzyowKvQupW4yMH9fGNxtsh6iJswRqOOfZYC4/giBO/gNsBvwr8uDW7t1nYo\r\n"
        "DYGHPpvnpxCM2mYfQFHq576/TmeYu1RZY29C4w8xYBlkAA8mDJfRhMCmehk7cN5F\r\n"
        "JtyWRj2cZj/hOoI45TYDBChXpOlLZKIYiG1giY16vhCRi6zmPzEwv+tk156N6cGS\r\n"
        "Vm44jTQ/rs1sa0JSYjzUaYngoFdZC4OfxnIkQvUIA4TOFmPzNPEFdjcZsgbeEz4T\r\n"
        "cGHTBPK4R28F44qIMCtHRV55VMX53ev6P3hRddJb\r\n"
        "-----END CERTIFICATE-----\r\n";
    ```

1. Endre alle inkluder av `
<WiFiClient.h>` til `<WiFiClientSecure.h>`.

1. Endre alle `WiFiClient`-felt til `WiFiClientSecure`.

1. I hver klasse som har et `WiFiClientSecure`-felt, legg til en konstrukt√∏r og sett sertifikatet i den konstrukt√∏ren:

    ```cpp
    _client.setCACert(FUNCTIONS_CERTIFICATE);
    ```

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiserte oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.