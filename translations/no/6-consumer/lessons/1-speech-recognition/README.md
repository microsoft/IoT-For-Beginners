<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "6d6aa1be033625d201a190fc9c5cbfb4",
  "translation_date": "2025-08-27T21:02:30+00:00",
  "source_file": "6-consumer/lessons/1-speech-recognition/README.md",
  "language_code": "no"
}
-->
# Gjenkjenne tale med en IoT-enhet

![En sketchnote som gir en oversikt over denne leksjonen](../../../../../translated_images/lesson-21.e34de51354d6606fb5ee08d8c89d0222eea0a2a7aaf744a8805ae847c4f69dc4.no.jpg)

> Sketchnote av [Nitya Narasimhan](https://github.com/nitya). Klikk p√• bildet for en st√∏rre versjon.

Denne videoen gir en oversikt over Azure tale-tjenesten, et tema som vil bli dekket i denne leksjonen:

[![Hvordan komme i gang med Cognitive Services Speech-ressursen fra Microsoft Azure YouTube-kanalen](https://img.youtube.com/vi/iW0Fw0l3mrA/0.jpg)](https://www.youtube.com/watch?v=iW0Fw0l3mrA)

> üé• Klikk p√• bildet ovenfor for √• se videoen

## Quiz f√∏r leksjonen

[Quiz f√∏r leksjonen](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/41)

## Introduksjon

'Alexa, sett en timer p√• 12 minutter'

'Alexa, status p√• timeren'

'Alexa, sett en timer p√• 8 minutter kalt damp brokkoli'

Smartenheter blir stadig mer utbredt. Ikke bare som smarth√∏yttalere som HomePods, Echos og Google Homes, men ogs√• integrert i telefoner, klokker, og til og med lysarmaturer og termostater.

> üíÅ Jeg har minst 19 enheter i hjemmet mitt som har stemmeassistenter, og det er bare de jeg vet om!

Stemmestyring √∏ker tilgjengeligheten ved √• la personer med begrenset bevegelighet interagere med enheter. Enten det er en permanent funksjonsnedsettelse som √• v√¶re f√∏dt uten armer, en midlertidig skade som brukne armer, eller bare det √• ha hendene fulle med matvarer eller sm√• barn, kan det √• styre huset med stemmen i stedet for hendene √•pne opp en verden av muligheter. √Ö rope 'Hei Siri, lukk garasjeporten' mens du h√•ndterer en bleieskift og en rampete sm√•rolling kan v√¶re en liten, men effektiv forbedring i hverdagen.

En av de mest popul√¶re bruksomr√•dene for stemmeassistenter er √• sette timere, spesielt kj√∏kkentimere. √Ö kunne sette flere timere med bare stemmen er en stor hjelp p√• kj√∏kkenet ‚Äì ingen grunn til √• stoppe med √• kna deigen, r√∏re i suppen, eller vaske hendene for √• bruke en fysisk timer.

I denne leksjonen vil du l√¶re om hvordan du kan bygge stemmegjenkjenning inn i IoT-enheter. Du vil l√¶re om mikrofoner som sensorer, hvordan du fanger lyd fra en mikrofon koblet til en IoT-enhet, og hvordan du bruker AI til √• konvertere det som h√∏res til tekst. Gjennom resten av dette prosjektet vil du bygge en smart kj√∏kkentimer som kan sette timere med stemmen din p√• flere spr√•k.

I denne leksjonen dekker vi:

* [Mikrofoner](../../../../../6-consumer/lessons/1-speech-recognition)
* [Fange lyd fra IoT-enheten din](../../../../../6-consumer/lessons/1-speech-recognition)
* [Tale til tekst](../../../../../6-consumer/lessons/1-speech-recognition)
* [Konverter tale til tekst](../../../../../6-consumer/lessons/1-speech-recognition)

## Mikrofoner

Mikrofoner er analoge sensorer som konverterer lydb√∏lger til elektriske signaler. Vibrasjoner i luften f√•r komponenter i mikrofonen til √• bevege seg sm√• mengder, og dette for√•rsaker sm√• endringer i elektriske signaler. Disse endringene forsterkes deretter for √• generere et elektrisk output.

### Mikrofontyper

Mikrofoner finnes i en rekke typer:

* Dynamisk - Dynamiske mikrofoner har en magnet festet til en bevegelig membran som beveger seg i en spole av ledning og skaper en elektrisk str√∏m. Dette er motsatt av de fleste h√∏yttalere, som bruker en elektrisk str√∏m til √• bevege en magnet i en spole av ledning, og beveger en membran for √• skape lyd. Dette betyr at h√∏yttalere kan brukes som dynamiske mikrofoner, og dynamiske mikrofoner kan brukes som h√∏yttalere. I enheter som intercoms, der en bruker enten lytter eller snakker, men ikke begge deler, kan √©n enhet fungere som b√•de h√∏yttaler og mikrofon.

    Dynamiske mikrofoner trenger ikke str√∏m for √• fungere, det elektriske signalet skapes helt av mikrofonen.

    ![Patti Smith synger inn i en Shure SM58 (dynamisk kardioid-type) mikrofon](../../../../../translated_images/dynamic-mic.8babac890a2d80dfb0874b5bf37d4b851fe2aeb9da6fd72945746176978bf3bb.no.jpg)

* B√•nd - B√•ndmikrofoner ligner p√• dynamiske mikrofoner, bortsett fra at de har et metallb√•nd i stedet for en membran. Dette b√•ndet beveger seg i et magnetfelt og genererer en elektrisk str√∏m. Som dynamiske mikrofoner trenger ikke b√•ndmikrofoner str√∏m for √• fungere.

    ![Edmund Lowe, amerikansk skuespiller, st√•r ved en radiomikrofon (merket for (NBC) Blue Network), holder manus, 1942](../../../../../translated_images/ribbon-mic.eacc8e092c7441caee6d7a81e2f40e1675bf36269848964c7c09c9a9acb05127.no.jpg)

* Kondensator - Kondensatormikrofoner har en tynn metallmembran og en fast metall bakplate. Elektrisitet p√•f√∏res begge disse, og n√•r membranen vibrerer, endres den statiske ladningen mellom platene og genererer et signal. Kondensatormikrofoner trenger str√∏m for √• fungere ‚Äì kalt *Phantom power*.

    ![C451B sm√•-membran kondensatormikrofon fra AKG Acoustics](../../../../../translated_images/condenser-mic.6f6ed5b76ca19e0ec3fd0c544601542d4479a6cb7565db336de49fbbf69f623e.no.jpg)

* MEMS - Mikroelektromekaniske systemmikrofoner, eller MEMS, er mikrofoner p√• en chip. De har en trykkf√∏lsom membran etset p√• en silisiumchip, og fungerer p√• samme m√•te som en kondensatormikrofon. Disse mikrofonene kan v√¶re sv√¶rt sm√• og integrert i kretsl√∏p.

    ![En MEMS-mikrofon p√• et kretskort](../../../../../translated_images/mems-microphone.80574019e1f5e4d9ee72fed720ecd25a39fc2969c91355d17ebb24ba4159e4c4.no.png)

    P√• bildet ovenfor er chipen merket **LEFT** en MEMS-mikrofon, med en liten membran mindre enn en millimeter bred.

‚úÖ Gj√∏r litt research: Hvilke mikrofoner har du rundt deg ‚Äì enten i datamaskinen, telefonen, headsettet eller andre enheter? Hvilken type mikrofoner er de?

### Digital lyd

Lyd er et analogt signal som b√¶rer sv√¶rt fin-granulert informasjon. For √• konvertere dette signalet til digitalt, m√• lyden samples mange tusen ganger i sekundet.

> üéì Sampling inneb√¶rer √• konvertere lydsignalet til en digital verdi som representerer signalet p√• det tidspunktet.

![En linjediagram som viser et signal, med diskrete punkter p√• faste intervaller](../../../../../translated_images/sampling.6f4fadb3f2d9dfe7618f9edfe75a350e6b3f74293ec84f02ab69c19d2afe3d73.no.png)

Digital lyd samples ved hjelp av Pulse Code Modulation, eller PCM. PCM inneb√¶rer √• lese spenningen i signalet og velge den n√¶rmeste diskrete verdien til den spenningen ved hjelp av en definert st√∏rrelse.

> üíÅ Du kan tenke p√• PCM som sensorversjonen av pulsbreddemodulasjon, eller PWM (PWM ble dekket tilbake i [leksjon 3 av introduksjonsprosjektet](../../../1-getting-started/lessons/3-sensors-and-actuators/README.md#pulse-width-modulation)). PCM inneb√¶rer √• konvertere et analogt signal til digitalt, PWM inneb√¶rer √• konvertere et digitalt signal til analogt.

For eksempel tilbyr de fleste str√∏mmetjenester for musikk 16-bit eller 24-bit lyd. Dette betyr at de konverterer spenningen til en verdi som passer inn i et 16-bit heltall eller 24-bit heltall. 16-bit lyd passer verdien inn i et tall som varierer fra -32,768 til 32,767, 24-bit er i omr√•det ‚àí8,388,608 til 8,388,607. Jo flere biter, desto n√¶rmere er samplingen det v√•re √∏rer faktisk h√∏rer.

> üíÅ Du har kanskje h√∏rt om 8-bit lyd, ofte referert til som LoFi. Dette er lyd samplet med bare 8 biter, alts√• -128 til 127. Den f√∏rste datamaskinlyden var begrenset til 8 biter p√• grunn av maskinvarebegrensninger, s√• dette sees ofte i retrospill.

Disse samplene tas mange tusen ganger per sekund, ved hjelp av veldefinerte samplingsfrekvenser m√•lt i KHz (tusenvis av avlesninger per sekund). Str√∏mmetjenester for musikk bruker 48KHz for de fleste lydspor, men noen 'lossless' lyd bruker opptil 96KHz eller til og med 192KHz. Jo h√∏yere samplingsfrekvens, desto n√¶rmere originalen vil lyden v√¶re, opp til et visst punkt. Det er debatt om mennesker kan merke forskjellen over 48KHz.

‚úÖ Gj√∏r litt research: Hvis du bruker en str√∏mmetjeneste for musikk, hvilken samplingsfrekvens og st√∏rrelse bruker den? Hvis du bruker CD-er, hva er samplingsfrekvensen og st√∏rrelsen p√• CD-lyd?

Det finnes en rekke forskjellige formater for lyddata. Du har sikkert h√∏rt om mp3-filer ‚Äì lyddata som er komprimert for √• gj√∏re dem mindre uten √• miste kvalitet. Ukomprimert lyd lagres ofte som en WAV-fil ‚Äì dette er en fil med 44 bytes med headerinformasjon, etterfulgt av r√• lyddata. Headeren inneholder informasjon som samplingsfrekvensen (for eksempel 16000 for 16KHz) og samplingsst√∏rrelsen (16 for 16-bit), og antall kanaler. Etter headeren inneholder WAV-filen de r√• lyddataene.

> üéì Kanaler refererer til hvor mange forskjellige lydstr√∏mmer som utgj√∏r lyden. For eksempel, for stereo lyd med venstre og h√∏yre, vil det v√¶re 2 kanaler. For 7.1 surroundlyd for et hjemmekinosystem vil dette v√¶re 8.

### Lyddatast√∏rrelse

Lyddata er relativt store. For eksempel, √• fange ukomprimert 16-bit lyd ved 16KHz (en god nok rate for bruk med tale-til-tekst-modeller), tar 32KB med data for hvert sekund med lyd:

* 16-bit betyr 2 bytes per sample (1 byte er 8 biter).
* 16KHz er 16,000 samples per sekund.
* 16,000 x 2 bytes = 32,000 bytes per sekund.

Dette h√∏res ut som en liten mengde data, men hvis du bruker en mikrokontroller med begrenset minne, kan dette v√¶re mye. For eksempel har Wio Terminal 192KB med minne, og det m√• lagre programkode og variabler. Selv om programkoden din var liten, kunne du ikke fange mer enn 5 sekunder med lyd.

Mikrokontrollere kan f√• tilgang til ekstra lagring, som SD-kort eller flashminne. N√•r du bygger en IoT-enhet som fanger lyd, m√• du s√∏rge for at du ikke bare har ekstra lagring, men at koden din skriver lyden som fanges fra mikrofonen direkte til den lagringen, og n√•r du sender den til skyen, streamer fra lagring til webforesp√∏rselen. P√• den m√•ten kan du unng√• √• g√• tom for minne ved √• pr√∏ve √• holde hele lydblokken i minnet samtidig.

## Fange lyd fra IoT-enheten din

IoT-enheten din kan kobles til en mikrofon for √• fange lyd, klar for konvertering til tekst. Den kan ogs√• kobles til h√∏yttalere for √• spille av lyd. I senere leksjoner vil dette brukes til √• gi lydtilbakemelding, men det er nyttig √• sette opp h√∏yttalere n√• for √• teste mikrofonen.

### Oppgave - konfigurer mikrofonen og h√∏yttalerne dine

F√∏lg den relevante guiden for √• konfigurere mikrofonen og h√∏yttalerne for IoT-enheten din:

* [Arduino - Wio Terminal](wio-terminal-microphone.md)
* [Enkeltkortdatamaskin - Raspberry Pi](pi-microphone.md)
* [Enkeltkortdatamaskin - Virtuell enhet](virtual-device-microphone.md)

### Oppgave - fang lyd

F√∏lg den relevante guiden for √• fange lyd p√• IoT-enheten din:

* [Arduino - Wio Terminal](wio-terminal-audio.md)
* [Enkeltkortdatamaskin - Raspberry Pi](pi-audio.md)
* [Enkeltkortdatamaskin - Virtuell enhet](virtual-device-audio.md)

## Tale til tekst

Tale til tekst, eller talegjenkjenning, inneb√¶rer √• bruke AI til √• konvertere ord i et lydsignal til tekst.

### Talemodeller

For √• konvertere tale til tekst, grupperes pr√∏ver fra lydsignalet sammen og mates inn i en maskinl√¶ringsmodell basert p√• et Recurrent Neural Network (RNN). Dette er en type maskinl√¶ringsmodell som kan bruke tidligere data til √• ta en beslutning om innkommende data. For eksempel kan RNN oppdage √©n blokk med lydpr√∏ver som lyden 'Hel', og n√•r den mottar en annen som den tror er lyden 'lo', kan den kombinere dette med den forrige lyden, finne ut at 'Hello' er et gyldig ord og velge det som resultat.

ML-modeller aksepterer alltid data av samme st√∏rrelse hver gang. Bildesorteringsmodellen du bygde i en tidligere leksjon endrer st√∏rrelsen p√• bilder til en fast st√∏rrelse og prosesserer dem. Det samme gjelder talemodeller, de m√• prosessere faste st√∏rrelser av lydblokker. Talemodellene m√• kunne kombinere resultatene av flere prediksjoner for √• f√• svaret, slik at de kan skille mellom 'Hi' og 'Highway', eller 'flock' og 'floccinaucinihilipilification'.

Talemodeller er ogs√• avanserte nok til √• forst√• kontekst, og kan korrigere ordene de oppdager etter hvert som flere lyder prosesseres. For eksempel, hvis du sier "Jeg dro til butikken for √• kj√∏pe to bananer og et eple ogs√•", vil du bruke tre ord som h√∏res like ut, men staves forskjellig ‚Äì to, two og too. Talemodeller er i stand til √• forst√• konteksten og bruke riktig stavem√•te for ordet.
üíÅ Noen talegjenkjenningstjenester tillater tilpasning for √• fungere bedre i st√∏yende milj√∏er som fabrikker, eller med bransjespesifikke ord som kjemiske navn. Disse tilpasningene trenes ved √• gi eksempellyd og en transkripsjon, og fungerer ved hjelp av transfer learning, p√• samme m√•te som du trente en bildegjenkjenner med bare noen f√• bilder i en tidligere leksjon.
### Personvern

N√•r du bruker tale-til-tekst p√• en forbruker-IoT-enhet, er personvern utrolig viktig. Disse enhetene lytter kontinuerlig til lyd, og som forbruker √∏nsker du ikke at alt du sier skal sendes til skyen og konverteres til tekst. Ikke bare vil dette bruke mye internettb√•ndbredde, det har ogs√• store personvernutfordringer, spesielt n√•r noen produsenter av smarte enheter tilfeldig velger lyd for [mennesker √• validere mot den genererte teksten for √• forbedre modellen deres](https://www.theverge.com/2019/4/10/18305378/amazon-alexa-ai-voice-assistant-annotation-listen-private-recordings).

Du vil kun at din smarte enhet skal sende lyd til skyen for behandling n√•r du bruker den, ikke n√•r den h√∏rer lyd i hjemmet ditt, lyd som kan inkludere private m√∏ter eller intime interaksjoner. M√•ten de fleste smarte enheter fungerer p√• er med et *aktiveringsord*, en n√∏kkelfrase som "Alexa", "Hey Siri" eller "OK Google" som f√•r enheten til √• 'v√•kne' og lytte til det du sier frem til den oppdager en pause i talen din, som indikerer at du er ferdig med √• snakke til enheten.

> üéì Aktiveringsordgjenkjenning kalles ogs√• *Keyword spotting* eller *Keyword recognition*.

Disse aktiveringsordene oppdages p√• enheten, ikke i skyen. Disse smarte enhetene har sm√• AI-modeller som kj√∏rer p√• enheten og lytter etter aktiveringsordet, og n√•r det oppdages, begynner de √• str√∏mme lyden til skyen for gjenkjenning. Disse modellene er sv√¶rt spesialiserte og lytter kun etter aktiveringsordet.

> üíÅ Noen teknologiselskaper legger til mer personvern i enhetene sine og utf√∏rer noe av tale-til-tekst-konverteringen p√• enheten. Apple har annonsert at som en del av deres 2021 iOS- og macOS-oppdateringer vil de st√∏tte tale-til-tekst-konvertering p√• enheten og kunne h√•ndtere mange foresp√∏rsler uten √• bruke skyen. Dette er mulig takket v√¶re kraftige prosessorer i enhetene deres som kan kj√∏re ML-modeller.

‚úÖ Hva mener du er de personvern- og etiske implikasjonene ved √• lagre lyd som sendes til skyen? B√∏r denne lyden lagres, og i s√• fall, hvordan? Synes du bruken av opptak for rettsh√•ndhevelse er en god avveining for tap av personvern?

Aktiveringsordgjenkjenning bruker vanligvis en teknikk kjent som TinyML, som inneb√¶rer √• konvertere ML-modeller slik at de kan kj√∏re p√• mikrokontrollere. Disse modellene er sm√• i st√∏rrelse og bruker sv√¶rt lite str√∏m.

For √• unng√• kompleksiteten ved √• trene og bruke en aktiveringsordmodell, vil den smarte timeren du bygger i denne leksjonen bruke en knapp for √• aktivere talegjenkjenning.

> üíÅ Hvis du vil pr√∏ve √• lage en aktiveringsordmodell som kan kj√∏re p√• Wio Terminal eller Raspberry Pi, kan du sjekke ut denne [veiledningen om √• svare p√• stemmen din fra Edge Impulse](https://docs.edgeimpulse.com/docs/responding-to-your-voice). Hvis du vil bruke datamaskinen din til dette, kan du pr√∏ve [komme i gang med Custom Keyword quickstart p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn).

## Konverter tale til tekst

![Logo for taletjenester](../../../../../translated_images/azure-speech-logo.a1f08c4befb0159f2cb5d692d3baf5b599e7b44759d316da907bda1508f46a4a.no.png)

Akkurat som med bildeklassifisering i et tidligere prosjekt, finnes det forh√•ndsbygde AI-tjenester som kan ta tale som en lydfil og konvertere den til tekst. En slik tjeneste er Speech Service, en del av Cognitive Services, forh√•ndsbygde AI-tjenester du kan bruke i appene dine.

### Oppgave - konfigurer en tale-AI-ressurs

1. Opprett en ressursgruppe for dette prosjektet kalt `smart-timer`.

1. Bruk f√∏lgende kommando for √• opprette en gratis taleressurs:

    ```sh
    az cognitiveservices account create --name smart-timer \
                                        --resource-group smart-timer \
                                        --kind SpeechServices \
                                        --sku F0 \
                                        --yes \
                                        --location <location>
    ```

    Erstatt `<location>` med stedet du brukte da du opprettet ressursgruppen.

1. Du trenger en API-n√∏kkel for √• f√• tilgang til taleressursen fra koden din. Kj√∏r f√∏lgende kommando for √• hente n√∏kkelen:

    ```sh
    az cognitiveservices account keys list --name smart-timer \
                                           --resource-group smart-timer \
                                           --output table
    ```

    Ta en kopi av en av n√∏klene.

### Oppgave - konverter tale til tekst

F√∏lg den relevante veiledningen for √• konvertere tale til tekst p√• din IoT-enhet:

* [Arduino - Wio Terminal](wio-terminal-speech-to-text.md)
* [Enkeltkortdatamaskin - Raspberry Pi](pi-speech-to-text.md)
* [Enkeltkortdatamaskin - Virtuell enhet](virtual-device-speech-to-text.md)

---

## üöÄ Utfordring

Talegjenkjenning har eksistert lenge og forbedres kontinuerlig. Unders√∏k dagens kapabiliteter og sammenlign hvordan disse har utviklet seg over tid, inkludert hvor n√∏yaktige maskintranskripsjoner er sammenlignet med menneskelige.

Hva tror du fremtiden bringer for talegjenkjenning?

## Quiz etter forelesning

[Quiz etter forelesning](https://black-meadow-040d15503.1.azurestaticapps.net/quiz/42)

## Gjennomgang og selvstudium

* Les om de forskjellige mikrofontypene og hvordan de fungerer i [artikkelen om forskjellen mellom dynamiske og kondensatormikrofoner p√• Musician's HQ](https://musicianshq.com/whats-the-difference-between-dynamic-and-condenser-microphones/).
* Les mer om Cognitive Services taletjeneste i [dokumentasjonen for taletjenesten p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/?WT.mc_id=academic-17441-jabenn).
* Les om aktiveringsordgjenkjenning i [dokumentasjonen for aktiveringsordgjenkjenning p√• Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/keyword-recognition-overview?WT.mc_id=academic-17441-jabenn).

## Oppgave

[](assignment.md)

---

**Ansvarsfraskrivelse**:  
Dette dokumentet er oversatt ved hjelp av AI-oversettelsestjenesten [Co-op Translator](https://github.com/Azure/co-op-translator). Selv om vi streber etter n√∏yaktighet, v√¶r oppmerksom p√• at automatiske oversettelser kan inneholde feil eller un√∏yaktigheter. Det originale dokumentet p√• sitt opprinnelige spr√•k b√∏r anses som den autoritative kilden. For kritisk informasjon anbefales profesjonell menneskelig oversettelse. Vi er ikke ansvarlige for misforst√•elser eller feiltolkninger som oppst√•r ved bruk av denne oversettelsen.