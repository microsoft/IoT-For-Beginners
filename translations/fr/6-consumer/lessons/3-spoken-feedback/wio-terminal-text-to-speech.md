<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "a202fa5889790a3777bfc33dd9f4b459",
  "translation_date": "2025-08-25T00:13:11+00:00",
  "source_file": "6-consumer/lessons/3-spoken-feedback/wio-terminal-text-to-speech.md",
  "language_code": "fr"
}
-->
# Conversion de texte en parole - Wio Terminal

Dans cette partie de la le√ßon, vous allez convertir du texte en parole pour fournir un retour vocal.

## Texte en parole

Le SDK des services vocaux que vous avez utilis√© dans la le√ßon pr√©c√©dente pour convertir la parole en texte peut √©galement √™tre utilis√© pour convertir du texte en parole.

## Obtenir une liste de voix

Lors de la demande de synth√®se vocale, vous devez sp√©cifier la voix √† utiliser, car la parole peut √™tre g√©n√©r√©e avec une vari√©t√© de voix diff√©rentes. Chaque langue prend en charge plusieurs voix, et vous pouvez obtenir la liste des voix disponibles pour chaque langue √† partir du SDK des services vocaux. Cependant, les limitations des microcontr√¥leurs entrent en jeu ici : l'appel pour obtenir la liste des voix disponibles g√©n√®re un document JSON de plus de 77 Ko, bien trop volumineux pour √™tre trait√© par le Wio Terminal. Au moment de la r√©daction, la liste compl√®te contient 215 voix, chacune d√©finie par un document JSON comme celui-ci :

```json
{
    "Name": "Microsoft Server Speech Text to Speech Voice (en-US, AriaNeural)",
    "DisplayName": "Aria",
    "LocalName": "Aria",
    "ShortName": "en-US-AriaNeural",
    "Gender": "Female",
    "Locale": "en-US",
    "StyleList": [
        "chat",
        "customerservice",
        "narration-professional",
        "newscast-casual",
        "newscast-formal",
        "cheerful",
        "empathetic"
    ],
    "SampleRateHertz": "24000",
    "VoiceType": "Neural",
    "Status": "GA"
}
```

Ce JSON correspond √† la voix **Aria**, qui propose plusieurs styles de voix. Pour convertir du texte en parole, seule l'information `shortname`, `en-US-AriaNeural`, est n√©cessaire.

Au lieu de t√©l√©charger et de d√©coder cette liste compl√®te sur votre microcontr√¥leur, vous devrez √©crire un peu de code sans serveur pour r√©cup√©rer la liste des voix pour la langue que vous utilisez, et appeler ce code depuis votre Wio Terminal. Votre code pourra ensuite choisir une voix appropri√©e dans la liste, comme la premi√®re qu'il trouve.

### T√¢che - Cr√©er une fonction sans serveur pour obtenir une liste de voix

1. Ouvrez votre projet `smart-timer-trigger` dans VS Code, et ouvrez le terminal en vous assurant que l'environnement virtuel est activ√©. Sinon, terminez et recr√©ez le terminal.

1. Ouvrez le fichier `local.settings.json` et ajoutez les param√®tres pour la cl√© API des services vocaux et la localisation :

    ```json
    "SPEECH_KEY": "<key>",
    "SPEECH_LOCATION": "<location>"
    ```

    Remplacez `<key>` par la cl√© API de votre ressource de service vocal. Remplacez `<location>` par la localisation utilis√©e lors de la cr√©ation de la ressource de service vocal.

1. Ajoutez un nouveau d√©clencheur HTTP √† cette application appel√© `get-voices` en utilisant la commande suivante dans le terminal de VS Code, √† la racine du projet de l'application de fonctions :

    ```sh
    func new --name get-voices --template "HTTP trigger"
    ```

    Cela cr√©era un d√©clencheur HTTP appel√© `get-voices`.

1. Remplacez le contenu du fichier `__init__.py` dans le dossier `get-voices` par ce qui suit :

    ```python
    import json
    import os
    import requests
    
    import azure.functions as func
    
    def main(req: func.HttpRequest) -> func.HttpResponse:
        location = os.environ['SPEECH_LOCATION']
        speech_key = os.environ['SPEECH_KEY']
    
        req_body = req.get_json()
        language = req_body['language']
    
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/voices/list'
    
        headers = {
            'Ocp-Apim-Subscription-Key': speech_key
        }
    
        response = requests.get(url, headers=headers)
        voices_json = json.loads(response.text)
    
        voices = filter(lambda x: x['Locale'].lower() == language.lower(), voices_json)
        voices = map(lambda x: x['ShortName'], voices)
    
        return func.HttpResponse(json.dumps(list(voices)), status_code=200)
    ```

    Ce code effectue une requ√™te HTTP √† l'endpoint pour obtenir les voix. La liste des voix est un grand bloc JSON contenant les voix pour toutes les langues. Les voix correspondant √† la langue sp√©cifi√©e dans le corps de la requ√™te sont filtr√©es, puis le `shortname` est extrait et renvoy√© sous forme de liste JSON. Le `shortname` est la valeur n√©cessaire pour convertir du texte en parole, donc seule cette valeur est renvoy√©e.

    > üíÅ Vous pouvez modifier le filtre si n√©cessaire pour s√©lectionner uniquement les voix souhait√©es.

    Cela r√©duit la taille des donn√©es de 77 Ko (au moment de la r√©daction) √† un document JSON beaucoup plus petit. Par exemple, pour les voix am√©ricaines, cela repr√©sente 408 octets.

1. Ex√©cutez votre application de fonctions localement. Vous pouvez ensuite l'appeler √† l'aide d'un outil comme curl, de la m√™me mani√®re que vous avez test√© votre d√©clencheur HTTP `text-to-timer`. Assurez-vous de passer votre langue dans le corps JSON :

    ```json
    {
        "language":"<language>"
    }
    ```

    Remplacez `<language>` par votre langue, comme `en-GB` ou `zh-CN`.

> üíÅ Vous pouvez trouver ce code dans le dossier [code-spoken-response/functions](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/functions).

### T√¢che - R√©cup√©rer la voix depuis votre Wio Terminal

1. Ouvrez le projet `smart-timer` dans VS Code s'il n'est pas d√©j√† ouvert.

1. Ouvrez le fichier d'en-t√™te `config.h` et ajoutez l'URL de votre application de fonctions :

    ```cpp
    const char *GET_VOICES_FUNCTION_URL = "<URL>";
    ```

    Remplacez `<URL>` par l'URL du d√©clencheur HTTP `get-voices` de votre application de fonctions. Cela sera identique √† la valeur de `TEXT_TO_TIMER_FUNCTION_URL`, sauf que le nom de la fonction sera `get-voices` au lieu de `text-to-timer`.

1. Cr√©ez un nouveau fichier dans le dossier `src` appel√© `text_to_speech.h`. Ce fichier sera utilis√© pour d√©finir une classe permettant de convertir du texte en parole.

1. Ajoutez les directives d'inclusion suivantes en haut du nouveau fichier `text_to_speech.h` :

    ```cpp
    #pragma once

    #include <Arduino.h>
    #include <ArduinoJson.h>
    #include <HTTPClient.h>
    #include <Seeed_FS.h>
    #include <SD/Seeed_SD.h>
    #include <WiFiClient.h>
    #include <WiFiClientSecure.h>
    
    #include "config.h"
    #include "speech_to_text.h"
    ```

1. Ajoutez ensuite le code suivant pour d√©clarer la classe `TextToSpeech`, ainsi qu'une instance qui pourra √™tre utilis√©e dans le reste de l'application :

    ```cpp
    class TextToSpeech
    {
    public:
    private:
    };
    
    TextToSpeech textToSpeech;
    ```

1. Pour appeler votre application de fonctions, vous devez d√©clarer un client WiFi. Ajoutez ce qui suit √† la section `private` de la classe :

    ```cpp
    WiFiClient _client;
    ```

1. Dans la section `private`, ajoutez un champ pour la voix s√©lectionn√©e :

    ```cpp
    String _voice;
    ```

1. Dans la section `public`, ajoutez une fonction `init` qui r√©cup√©rera la premi√®re voix :

    ```cpp
    void init()
    {
    }
    ```

1. Pour obtenir les voix, un document JSON doit √™tre envoy√© √† l'application de fonctions avec la langue. Ajoutez le code suivant √† la fonction `init` pour cr√©er ce document JSON :

    ```cpp
    DynamicJsonDocument doc(1024);
    doc["language"] = LANGUAGE;

    String body;
    serializeJson(doc, body);
    ```

1. Ensuite, cr√©ez un `HTTPClient`, puis utilisez-le pour appeler l'application de fonctions afin d'obtenir les voix, en postant le document JSON :

    ```cpp
    HTTPClient httpClient;
    httpClient.begin(_client, GET_VOICES_FUNCTION_URL);

    int httpResponseCode = httpClient.POST(body);
    ```

1. Ajoutez ensuite du code pour v√©rifier le code de r√©ponse, et si c'est 200 (succ√®s), extrayez la liste des voix et r√©cup√©rez la premi√®re de la liste :

    ```cpp
    if (httpResponseCode == 200)
    {
        String result = httpClient.getString();
        Serial.println(result);

        DynamicJsonDocument doc(1024);
        deserializeJson(doc, result.c_str());

        JsonArray obj = doc.as<JsonArray>();
        _voice = obj[0].as<String>();

        Serial.print("Using voice ");
        Serial.println(_voice);
    }
    else
    {
        Serial.print("Failed to get voices - error ");
        Serial.println(httpResponseCode);
    }
    ```

1. Apr√®s cela, terminez la connexion du client HTTP :

    ```cpp
    httpClient.end();
    ```

1. Ouvrez le fichier `main.cpp` et ajoutez la directive d'inclusion suivante en haut pour inclure ce nouveau fichier d'en-t√™te :

    ```cpp
    #include "text_to_speech.h"
    ```

1. Dans la fonction `setup`, sous l'appel √† `speechToText.init();`, ajoutez ce qui suit pour initialiser la classe `TextToSpeech` :

    ```cpp
    textToSpeech.init();
    ```

1. Compilez ce code, t√©l√©versez-le sur votre Wio Terminal et testez-le via le moniteur s√©rie. Assurez-vous que votre application de fonctions est en cours d'ex√©cution.

    Vous verrez la liste des voix disponibles renvoy√©e par l'application de fonctions, ainsi que la voix s√©lectionn√©e.

    ```output
    --- Available filters and text transformations: colorize, debug, default, direct, hexlify, log2file, nocontrol, printable, send_on_enter, time
    --- More details at http://bit.ly/pio-monitor-filters
    --- Miniterm on /dev/cu.usbmodem1101  9600,8,N,1 ---
    --- Quit: Ctrl+C | Menu: Ctrl+T | Help: Ctrl+T followed by Ctrl+H ---
    Connecting to WiFi..
    Connected!
    Got access token.
    ["en-US-JennyNeural", "en-US-JennyMultilingualNeural", "en-US-GuyNeural", "en-US-AriaNeural", "en-US-AmberNeural", "en-US-AnaNeural", "en-US-AshleyNeural", "en-US-BrandonNeural", "en-US-ChristopherNeural", "en-US-CoraNeural", "en-US-ElizabethNeural", "en-US-EricNeural", "en-US-JacobNeural", "en-US-MichelleNeural", "en-US-MonicaNeural", "en-US-AriaRUS", "en-US-BenjaminRUS", "en-US-GuyRUS", "en-US-ZiraRUS"]
    Using voice en-US-JennyNeural
    Ready.
    ```

## Convertir du texte en parole

Une fois que vous avez une voix √† utiliser, elle peut √™tre utilis√©e pour convertir du texte en parole. Les m√™mes limitations de m√©moire pour les voix s'appliquent √©galement lors de la conversion de texte en parole, donc vous devrez √©crire le fichier audio sur une carte SD pour qu'il puisse √™tre lu via le ReSpeaker.

> üíÅ Dans les le√ßons pr√©c√©dentes de ce projet, vous avez utilis√© la m√©moire flash pour stocker la parole captur√©e par le microphone. Cette le√ßon utilise une carte SD car il est plus facile de lire de l'audio √† partir de celle-ci en utilisant les biblioth√®ques audio de Seeed.

Il existe √©galement une autre limitation √† prendre en compte : les donn√©es audio disponibles √† partir du service vocal et les formats pris en charge par le Wio Terminal. Contrairement aux ordinateurs complets, les biblioth√®ques audio pour microcontr√¥leurs peuvent √™tre tr√®s limit√©es en termes de formats audio pris en charge. Par exemple, la biblioth√®que Seeed Arduino Audio qui permet de lire du son via le ReSpeaker ne prend en charge que l'audio avec un taux d'√©chantillonnage de 44,1 kHz. Les services vocaux Azure peuvent fournir de l'audio dans plusieurs formats, mais aucun d'entre eux n'utilise ce taux d'√©chantillonnage. Ils fournissent uniquement 8 kHz, 16 kHz, 24 kHz et 48 kHz. Cela signifie que l'audio doit √™tre r√©-√©chantillonn√© √† 44,1 kHz, ce qui n√©cessiterait plus de ressources que celles disponibles sur le Wio Terminal, en particulier en termes de m√©moire.

Lorsque vous devez manipuler des donn√©es de cette mani√®re, il est souvent pr√©f√©rable d'utiliser du code sans serveur, surtout si les donn√©es sont obtenues via un appel web. Le Wio Terminal peut appeler une fonction sans serveur, en passant le texte √† convertir, et la fonction sans serveur peut √† la fois appeler le service vocal pour convertir le texte en parole, ainsi que r√©-√©chantillonner l'audio au taux d'√©chantillonnage requis. Elle peut ensuite renvoyer l'audio dans un format que le Wio Terminal peut stocker sur la carte SD et lire via le ReSpeaker.

### T√¢che - Cr√©er une fonction sans serveur pour convertir du texte en parole

1. Ouvrez votre projet `smart-timer-trigger` dans VS Code, et ouvrez le terminal en vous assurant que l'environnement virtuel est activ√©. Sinon, terminez et recr√©ez le terminal.

1. Ajoutez un nouveau d√©clencheur HTTP √† cette application appel√© `text-to-speech` en utilisant la commande suivante dans le terminal de VS Code, √† la racine du projet de l'application de fonctions :

    ```sh
    func new --name text-to-speech --template "HTTP trigger"
    ```

    Cela cr√©era un d√©clencheur HTTP appel√© `text-to-speech`.

1. Le package Pip [librosa](https://librosa.org) contient des fonctions pour r√©-√©chantillonner l'audio, ajoutez-le donc au fichier `requirements.txt` :

    ```sh
    librosa
    ```

    Une fois ajout√©, installez les packages Pip en utilisant la commande suivante dans le terminal de VS Code :

    ```sh
    pip install -r requirements.txt
    ```

    > ‚ö†Ô∏è Si vous utilisez Linux, y compris Raspberry Pi OS, vous devrez peut-√™tre installer `libsndfile` avec la commande suivante :
    >
    > ```sh
    > sudo apt update
    > sudo apt install libsndfile1-dev
    > ```

1. Pour convertir du texte en parole, vous ne pouvez pas utiliser directement la cl√© API des services vocaux. Vous devez demander un jeton d'acc√®s, en utilisant la cl√© API pour authentifier la demande de jeton d'acc√®s. Ouvrez le fichier `__init__.py` du dossier `text-to-speech` et remplacez tout le code par ce qui suit :

    ```python
    import io
    import os
    import requests
    
    import librosa
    import soundfile as sf
    import azure.functions as func
    
    location = os.environ['SPEECH_LOCATION']
    speech_key = os.environ['SPEECH_KEY']
    
    def get_access_token():
        headers = {
            'Ocp-Apim-Subscription-Key': speech_key
        }
    
        token_endpoint = f'https://{location}.api.cognitive.microsoft.com/sts/v1.0/issuetoken'
        response = requests.post(token_endpoint, headers=headers)
        return str(response.text)
    ```

    Cela d√©finit des constantes pour la localisation et la cl√© des services vocaux, qui seront lues √† partir des param√®tres. Ensuite, la fonction `get_access_token` est d√©finie pour r√©cup√©rer un jeton d'acc√®s pour le service vocal.

1. Ajoutez ensuite le code suivant :

    ```python
    playback_format = 'riff-48khz-16bit-mono-pcm'

    def main(req: func.HttpRequest) -> func.HttpResponse:
        req_body = req.get_json()
        language = req_body['language']
        voice = req_body['voice']
        text = req_body['text']
    
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/v1'
    
        headers = {
            'Authorization': 'Bearer ' + get_access_token(),
            'Content-Type': 'application/ssml+xml',
            'X-Microsoft-OutputFormat': playback_format
        }
    
        ssml =  f'<speak version=\'1.0\' xml:lang=\'{language}\'>'
        ssml += f'<voice xml:lang=\'{language}\' name=\'{voice}\'>'
        ssml += text
        ssml += '</voice>'
        ssml += '</speak>'
    
        response = requests.post(url, headers=headers, data=ssml.encode('utf-8'))
    
        raw_audio, sample_rate = librosa.load(io.BytesIO(response.content), sr=48000)
        resampled = librosa.resample(raw_audio, sample_rate, 44100)
        
        output_buffer = io.BytesIO()
        sf.write(output_buffer, resampled, 44100, 'PCM_16', format='wav')
        output_buffer.seek(0)
    
        return func.HttpResponse(output_buffer.read(), status_code=200)
    ```

    Cela d√©finit le d√©clencheur HTTP qui convertit le texte en parole. Il extrait le texte √† convertir, la langue et la voix du corps JSON envoy√© avec la requ√™te, construit un SSML pour demander la synth√®se vocale, puis appelle l'API REST correspondante en s'authentifiant avec le jeton d'acc√®s. Cet appel API REST renvoie l'audio encod√© en fichier WAV mono 16 bits, 48 kHz, d√©fini par la valeur de `playback_format`, qui est envoy√©e √† l'appel API REST.

    Cet audio est ensuite r√©-√©chantillonn√© par `librosa` d'un taux d'√©chantillonnage de 48 kHz √† un taux d'√©chantillonnage de 44,1 kHz, puis cet audio est enregistr√© dans un tampon binaire qui est ensuite renvoy√©.

1. Ex√©cutez votre application de fonctions localement ou d√©ployez-la dans le cloud. Vous pouvez ensuite l'appeler √† l'aide d'un outil comme curl, de la m√™me mani√®re que vous avez test√© votre d√©clencheur HTTP `text-to-timer`. Assurez-vous de passer la langue, la voix et le texte dans le corps JSON :

    ```json
    {
        "language": "<language>",
        "voice": "<voice>",
        "text": "<text>"
    }
    ```

    Remplacez `<language>` par votre langue, comme `en-GB` ou `zh-CN`. Remplacez `<voice>` par la voix que vous souhaitez utiliser. Remplacez `<text>` par le texte que vous souhaitez convertir en parole. Vous pouvez enregistrer la sortie dans un fichier et la lire avec n'importe quel lecteur audio capable de lire des fichiers WAV.

    Par exemple, pour convertir "Hello" en parole en utilisant l'anglais am√©ricain avec la voix Jenny Neural, avec l'application de fonctions ex√©cut√©e localement, vous pouvez utiliser la commande curl suivante :

    ```sh
    curl -X GET 'http://localhost:7071/api/text-to-speech' \
         -H 'Content-Type: application/json' \
         -o hello.wav \
         -d '{
           "language":"en-US",
           "voice": "en-US-JennyNeural",
           "text": "Hello"
         }'
    ```

    Cela enregistrera l'audio dans `hello.wav` dans le r√©pertoire actuel.

> üíÅ Vous pouvez trouver ce code dans le dossier [code-spoken-response/functions](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/functions).

### T√¢che - R√©cup√©rer la parole depuis votre Wio Terminal

1. Ouvrez le projet `smart-timer` dans VS Code s'il n'est pas d√©j√† ouvert.

1. Ouvrez le fichier d'en-t√™te `config.h` et ajoutez l'URL de votre application de fonctions :

    ```cpp
    const char *TEXT_TO_SPEECH_FUNCTION_URL = "<URL>";
    ```

    Remplacez `<URL>` par l'URL du d√©clencheur HTTP `text-to-speech` de votre application de fonctions. Cela sera identique √† la valeur de `TEXT_TO_TIMER_FUNCTION_URL`, sauf que le nom de la fonction sera `text-to-speech` au lieu de `text-to-timer`.

1. Ouvrez le fichier d'en-t√™te `text_to_speech.h` et ajoutez la m√©thode suivante √† la section `public` de la classe `TextToSpeech` :

    ```cpp
    void convertTextToSpeech(String text)
    {
    }
    ```

1. Dans la m√©thode `convertTextToSpeech`, ajoutez le code suivant pour cr√©er le JSON √† envoyer √† l'application de fonctions :

    ```cpp
    DynamicJsonDocument doc(1024);
    doc["language"] = LANGUAGE;
    doc["voice"] = _voice;
    doc["text"] = text;

    String body;
    serializeJson(doc, body);
    ```

    Cela √©crit la langue, la voix et le texte dans le document JSON, puis le s√©rialise en une cha√Æne.

1. Ajoutez ensuite le code suivant pour appeler l'application de fonctions :

    ```cpp
    HTTPClient httpClient;
    httpClient.begin(_client, TEXT_TO_SPEECH_FUNCTION_URL);

    int httpResponseCode = httpClient.POST(body);
    ```

    Cela cr√©e un `HTTPClient`, puis effectue une requ√™te POST en utilisant le document JSON vers le d√©clencheur HTTP `text-to-speech`.

1. Si l'appel r√©ussit, les donn√©es binaires brutes renvoy√©es par l'appel de l'application de fonctions peuvent √™tre diffus√©es dans un fichier sur la carte SD. Ajoutez le code suivant pour le faire :

    ```cpp
    if (httpResponseCode == 200)
    {            
        File wav_file = SD.open("SPEECH.WAV", FILE_WRITE);
        httpClient.writeToStream(&wav_file);
        wav_file.close();
    }
    else
    {
        Serial.print("Failed to get speech - error ");
        Serial.println(httpResponseCode);
    }
    ```

    Ce code v√©rifie la r√©ponse, et si c'est 200 (succ√®s), les donn√©es binaires sont diffus√©es dans un fichier √† la racine de la carte SD appel√© `SPEECH.WAV`.

1. √Ä la fin de cette m√©thode, fermez la connexion HTTP :

    ```cpp
    httpClient.end();
    ```

1. Le texte √† prononcer peut maintenant √™tre converti en audio. Dans le fichier `main.cpp`, ajoutez la ligne suivante √† la fin de la fonction `say` pour convertir le texte √† dire en audio :
```cpp
    textToSpeech.convertTextToSpeech(text);
    ```

### T√¢che - lire de l'audio depuis votre Wio Terminal

**Bient√¥t disponible**

## D√©ployer votre application de fonctions dans le cloud

La raison pour laquelle on ex√©cute l'application de fonctions localement est que le package Pip `librosa` sur Linux a une d√©pendance √† une biblioth√®que qui n'est pas install√©e par d√©faut et devra √™tre install√©e avant que l'application de fonctions puisse fonctionner. Les applications de fonctions sont sans serveur - il n'y a pas de serveurs que vous pouvez g√©rer vous-m√™me, donc aucun moyen d'installer cette biblioth√®que √† l'avance.

La solution consiste √† d√©ployer votre application de fonctions en utilisant un conteneur Docker. Ce conteneur est d√©ploy√© par le cloud chaque fois qu'il doit lancer une nouvelle instance de votre application de fonctions (par exemple, lorsque la demande d√©passe les ressources disponibles, ou si l'application de fonctions n'a pas √©t√© utilis√©e depuis un moment et a √©t√© arr√™t√©e).

Vous pouvez trouver les instructions pour configurer une application de fonctions et la d√©ployer via Docker dans la [documentation sur la cr√©ation d'une fonction sur Linux en utilisant une image personnalis√©e sur Microsoft Docs](https://docs.microsoft.com/azure/azure-functions/functions-create-function-linux-custom-image?WT.mc_id=academic-17441-jabenn&tabs=bash%2Cazurecli&pivots=programming-language-python).

Une fois que cela a √©t√© d√©ploy√©, vous pouvez adapter votre code Wio Terminal pour acc√©der √† cette fonction :

1. Ajoutez le certificat Azure Functions √† `config.h` :

    ```cpp
    const char *FUNCTIONS_CERTIFICATE =
        "-----BEGIN CERTIFICATE-----\r\n"
        "MIIFWjCCBEKgAwIBAgIQDxSWXyAgaZlP1ceseIlB4jANBgkqhkiG9w0BAQsFADBa\r\n"
        "MQswCQYDVQQGEwJJRTESMBAGA1UEChMJQmFsdGltb3JlMRMwEQYDVQQLEwpDeWJl\r\n"
        "clRydXN0MSIwIAYDVQQDExlCYWx0aW1vcmUgQ3liZXJUcnVzdCBSb290MB4XDTIw\r\n"
        "MDcyMTIzMDAwMFoXDTI0MTAwODA3MDAwMFowTzELMAkGA1UEBhMCVVMxHjAcBgNV\r\n"
        "BAoTFU1pY3Jvc29mdCBDb3Jwb3JhdGlvbjEgMB4GA1UEAxMXTWljcm9zb2Z0IFJT\r\n"
        "QSBUTFMgQ0EgMDEwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQCqYnfP\r\n"
        "mmOyBoTzkDb0mfMUUavqlQo7Rgb9EUEf/lsGWMk4bgj8T0RIzTqk970eouKVuL5R\r\n"
        "IMW/snBjXXgMQ8ApzWRJCZbar879BV8rKpHoAW4uGJssnNABf2n17j9TiFy6BWy+\r\n"
        "IhVnFILyLNK+W2M3zK9gheiWa2uACKhuvgCca5Vw/OQYErEdG7LBEzFnMzTmJcli\r\n"
        "W1iCdXby/vI/OxbfqkKD4zJtm45DJvC9Dh+hpzqvLMiK5uo/+aXSJY+SqhoIEpz+\r\n"
        "rErHw+uAlKuHFtEjSeeku8eR3+Z5ND9BSqc6JtLqb0bjOHPm5dSRrgt4nnil75bj\r\n"
        "c9j3lWXpBb9PXP9Sp/nPCK+nTQmZwHGjUnqlO9ebAVQD47ZisFonnDAmjrZNVqEX\r\n"
        "F3p7laEHrFMxttYuD81BdOzxAbL9Rb/8MeFGQjE2Qx65qgVfhH+RsYuuD9dUw/3w\r\n"
        "ZAhq05yO6nk07AM9c+AbNtRoEcdZcLCHfMDcbkXKNs5DJncCqXAN6LhXVERCw/us\r\n"
        "G2MmCMLSIx9/kwt8bwhUmitOXc6fpT7SmFvRAtvxg84wUkg4Y/Gx++0j0z6StSeN\r\n"
        "0EJz150jaHG6WV4HUqaWTb98Tm90IgXAU4AW2GBOlzFPiU5IY9jt+eXC2Q6yC/Zp\r\n"
        "TL1LAcnL3Qa/OgLrHN0wiw1KFGD51WRPQ0Sh7QIDAQABo4IBJTCCASEwHQYDVR0O\r\n"
        "BBYEFLV2DDARzseSQk1Mx1wsyKkM6AtkMB8GA1UdIwQYMBaAFOWdWTCCR1jMrPoI\r\n"
        "VDaGezq1BE3wMA4GA1UdDwEB/wQEAwIBhjAdBgNVHSUEFjAUBggrBgEFBQcDAQYI\r\n"
        "KwYBBQUHAwIwEgYDVR0TAQH/BAgwBgEB/wIBADA0BggrBgEFBQcBAQQoMCYwJAYI\r\n"
        "KwYBBQUHMAGGGGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTA6BgNVHR8EMzAxMC+g\r\n"
        "LaArhilodHRwOi8vY3JsMy5kaWdpY2VydC5jb20vT21uaXJvb3QyMDI1LmNybDAq\r\n"
        "BgNVHSAEIzAhMAgGBmeBDAECATAIBgZngQwBAgIwCwYJKwYBBAGCNyoBMA0GCSqG\r\n"
        "SIb3DQEBCwUAA4IBAQCfK76SZ1vae4qt6P+dTQUO7bYNFUHR5hXcA2D59CJWnEj5\r\n"
        "na7aKzyowKvQupW4yMH9fGNxtsh6iJswRqOOfZYC4/giBO/gNsBvwr8uDW7t1nYo\r\n"
        "DYGHPpvnpxCM2mYfQFHq576/TmeYu1RZY29C4w8xYBlkAA8mDJfRhMCmehk7cN5F\r\n"
        "JtyWRj2cZj/hOoI45TYDBChXpOlLZKIYiG1giY16vhCRi6zmPzEwv+tk156N6cGS\r\n"
        "Vm44jTQ/rs1sa0JSYjzUaYngoFdZC4OfxnIkQvUIA4TOFmPzNPEFdjcZsgbeEz4T\r\n"
        "cGHTBPK4R28F44qIMCtHRV55VMX53ev6P3hRddJb\r\n"
        "-----END CERTIFICATE-----\r\n";
    ```

1. Remplacez toutes les inclusions de `<WiFiClient.h>` par `<WiFiClientSecure.h>`.

1. Remplacez tous les champs `WiFiClient` par `WiFiClientSecure`.

1. Dans chaque classe qui contient un champ `WiFiClientSecure`, ajoutez un constructeur et d√©finissez le certificat dans ce constructeur :

    ```cpp
    _client.setCACert(FUNCTIONS_CERTIFICATE);
    ```

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.