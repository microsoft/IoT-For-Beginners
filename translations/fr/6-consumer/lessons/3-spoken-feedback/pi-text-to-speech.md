<!--
CO_OP_TRANSLATOR_METADATA:
{
  "original_hash": "606f3af1c78e3741e48ce77c31cea626",
  "translation_date": "2025-08-25T00:11:04+00:00",
  "source_file": "6-consumer/lessons/3-spoken-feedback/pi-text-to-speech.md",
  "language_code": "fr"
}
-->
# Texte en parole - Raspberry Pi

Dans cette partie de la le√ßon, vous allez √©crire du code pour convertir du texte en parole en utilisant le service vocal.

## Convertir du texte en parole avec le service vocal

Le texte peut √™tre envoy√© au service vocal via l'API REST pour obtenir un fichier audio qui peut √™tre lu sur votre appareil IoT. Lors de la demande de conversion en parole, vous devez sp√©cifier la voix √† utiliser, car la parole peut √™tre g√©n√©r√©e avec une vari√©t√© de voix diff√©rentes.

Chaque langue prend en charge une gamme de voix diff√©rentes, et vous pouvez effectuer une requ√™te REST aupr√®s du service vocal pour obtenir la liste des voix disponibles pour chaque langue.

### T√¢che - obtenir une voix

1. Ouvrez le projet `smart-timer` dans VS Code.

1. Ajoutez le code suivant au-dessus de la fonction `say` pour demander la liste des voix disponibles pour une langue :

    ```python
    def get_voice():
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/voices/list'
    
        headers = {
            'Authorization': 'Bearer ' + get_access_token()
        }
    
        response = requests.get(url, headers=headers)
        voices_json = json.loads(response.text)
    
        first_voice = next(x for x in voices_json if x['Locale'].lower() == language.lower() and x['VoiceType'] == 'Neural')
        return first_voice['ShortName']
    
    voice = get_voice()
    print(f'Using voice {voice}')
    ```

    Ce code d√©finit une fonction appel√©e `get_voice` qui utilise le service vocal pour obtenir une liste de voix. Il trouve ensuite la premi√®re voix correspondant √† la langue utilis√©e.

    Cette fonction est ensuite appel√©e pour stocker la premi√®re voix, et le nom de la voix est affich√© dans la console. Cette voix peut √™tre demand√©e une fois et la valeur utilis√©e pour chaque appel de conversion de texte en parole.

    > üíÅ Vous pouvez obtenir la liste compl√®te des voix disponibles dans la [documentation sur le support des langues et des voix sur Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/language-support?WT.mc_id=academic-17441-jabenn#text-to-speech). Si vous souhaitez utiliser une voix sp√©cifique, vous pouvez supprimer cette fonction et coder en dur le nom de la voix √† partir de cette documentation. Par exemple :
    >
    > ```python
    > voice = 'hi-IN-SwaraNeural'
    > ```

### T√¢che - convertir du texte en parole

1. En dessous, d√©finissez une constante pour le format audio √† r√©cup√©rer aupr√®s des services vocaux. Lorsque vous demandez un fichier audio, vous pouvez le faire dans une gamme de formats diff√©rents.

    ```python
    playback_format = 'riff-48khz-16bit-mono-pcm'
    ```

    Le format que vous pouvez utiliser d√©pend de votre mat√©riel. Si vous obtenez des erreurs de type `Invalid sample rate` lors de la lecture de l'audio, modifiez cette valeur. Vous pouvez trouver la liste des valeurs prises en charge dans la [documentation de l'API REST pour la conversion de texte en parole sur Microsoft Docs](https://docs.microsoft.com/azure/cognitive-services/speech-service/rest-text-to-speech?WT.mc_id=academic-17441-jabenn#audio-outputs). Vous devrez utiliser un format audio `riff`, et les valeurs √† essayer sont `riff-16khz-16bit-mono-pcm`, `riff-24khz-16bit-mono-pcm` et `riff-48khz-16bit-mono-pcm`.

1. En dessous, d√©clarez une fonction appel√©e `get_speech` qui convertira le texte en parole en utilisant l'API REST du service vocal :

    ```python
    def get_speech(text):
    ```

1. Dans la fonction `get_speech`, d√©finissez l'URL √† appeler et les en-t√™tes √† transmettre :

    ```python
        url = f'https://{location}.tts.speech.microsoft.com/cognitiveservices/v1'
    
        headers = {
            'Authorization': 'Bearer ' + get_access_token(),
            'Content-Type': 'application/ssml+xml',
            'X-Microsoft-OutputFormat': playback_format
        }
    ```

    Cela configure les en-t√™tes pour utiliser un jeton d'acc√®s g√©n√©r√©, d√©finit le contenu en SSML et sp√©cifie le format audio requis.

1. En dessous, d√©finissez le SSML √† envoyer √† l'API REST :

    ```python
    ssml =  f'<speak version=\'1.0\' xml:lang=\'{language}\'>'
    ssml += f'<voice xml:lang=\'{language}\' name=\'{voice}\'>'
    ssml += text
    ssml += '</voice>'
    ssml += '</speak>'
    ```

    Ce SSML d√©finit la langue et la voix √† utiliser, ainsi que le texte √† convertir.

1. Enfin, ajoutez du code dans cette fonction pour effectuer la requ√™te REST et retourner les donn√©es audio binaires :

    ```python
    response = requests.post(url, headers=headers, data=ssml.encode('utf-8'))
    return io.BytesIO(response.content)
    ```

### T√¢che - lire l'audio

1. En dessous de la fonction `get_speech`, d√©finissez une nouvelle fonction pour lire l'audio retourn√© par l'appel √† l'API REST :

    ```python
    def play_speech(speech):
    ```

1. Le `speech` pass√© √† cette fonction sera les donn√©es audio binaires retourn√©es par l'API REST. Utilisez le code suivant pour ouvrir cela en tant que fichier wave et le transmettre √† PyAudio pour lire l'audio :

    ```python
    def play_speech(speech):
        with wave.open(speech, 'rb') as wave_file:
            stream = audio.open(format=audio.get_format_from_width(wave_file.getsampwidth()),
                                channels=wave_file.getnchannels(),
                                rate=wave_file.getframerate(),
                                output_device_index=speaker_card_number,
                                output=True)

            data = wave_file.readframes(4096)

            while len(data) > 0:
                stream.write(data)
                data = wave_file.readframes(4096)

            stream.stop_stream()
            stream.close()
    ```

    Ce code utilise un flux PyAudio, comme pour capturer de l'audio. La diff√©rence ici est que le flux est configur√© comme un flux de sortie, et les donn√©es sont lues √† partir des donn√©es audio et pouss√©es dans le flux.

    Plut√¥t que de coder en dur les d√©tails du flux tels que le taux d'√©chantillonnage, ils sont lus √† partir des donn√©es audio.

1. Remplacez le contenu de la fonction `say` par le suivant :

    ```python
    speech = get_speech(text)
    play_speech(speech)
    ```

    Ce code convertit le texte en parole sous forme de donn√©es audio binaires, et lit l'audio.

1. Ex√©cutez l'application, et assurez-vous que l'application fonctionnelle est √©galement en cours d'ex√©cution. Configurez des minuteries, et vous entendrez une r√©ponse vocale indiquant que votre minuterie a √©t√© configur√©e, puis une autre r√©ponse vocale lorsque la minuterie est termin√©e.

    Si vous obtenez des erreurs de type `Invalid sample rate`, modifiez le `playback_format` comme d√©crit ci-dessus.

> üíÅ Vous pouvez trouver ce code dans le dossier [code-spoken-response/pi](../../../../../6-consumer/lessons/3-spoken-feedback/code-spoken-response/pi).

üòÄ Votre programme de minuterie est un succ√®s !

**Avertissement** :  
Ce document a √©t√© traduit √† l'aide du service de traduction automatique [Co-op Translator](https://github.com/Azure/co-op-translator). Bien que nous nous efforcions d'assurer l'exactitude, veuillez noter que les traductions automatis√©es peuvent contenir des erreurs ou des inexactitudes. Le document original dans sa langue d'origine doit √™tre consid√©r√© comme la source faisant autorit√©. Pour des informations critiques, il est recommand√© de recourir √† une traduction humaine professionnelle. Nous d√©clinons toute responsabilit√© en cas de malentendus ou d'interpr√©tations erron√©es r√©sultant de l'utilisation de cette traduction.